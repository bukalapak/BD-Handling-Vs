{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Volume with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Apache Spark to perform word count on product names after tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 PT Bukalapak.com\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(\"Python %s\" % sys.version)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark 2.4.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(\"PySpark %s\" % pyspark.__version__)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform 1.0.8\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(\"platform %s\" % platform.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS Linux-4.15.0-65-generic-x86_64-with-debian-buster-sid\n"
     ]
    }
   ],
   "source": [
    "print(\"OS\", platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow %s\" % tf.__version__)\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.4\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_222\n",
      "Branch \n",
      "Compiled by user  on 2019-08-27T21:21:38Z\n",
      "Revision \n",
      "Url \n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/spark/bin/spark-submit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Word Count using Notebook (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://25dd8860074f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>bukalapak-core-ai.big-data-3v.volume-spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=bukalapak-core-ai.big-data-3v.volume-spark>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and output URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/home/jovyan/work/data/product_names_sample/product_names.rdd'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_text_filename = \\\n",
    "    \"file:/home/jovyan/work/\" + \\\n",
    "    \"data/product_names_sample/\" + \\\n",
    "    \"product_names.rdd\"\n",
    "product_names_text_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/home/jovyan/work/data/product_names_sample/product_names_word_count_nb.orc'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_word_count_nb_orc_filename = \\\n",
    "    \"file:/home/jovyan/work/\" + \\\n",
    "    \"data/product_names_sample/\" + \\\n",
    "    \"product_names_word_count_nb.orc\"\n",
    "product_names_word_count_nb_orc_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_df = spark.read.text(product_names_text_filename)\n",
    "product_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='DAILY LIFE OF SCHOLAR SHINJIRO KATSURAGI 01: LC-MeruyaBookStore'),\n",
       " Row(value='Ready Stock Kulot Cantik'),\n",
       " Row(value='Sepaket Tas Dompet Sepatu Jam Kacamata Kalung'),\n",
       " Row(value='Baterai HP Pavilion DV3 2000 Compaq Presario CQ35   Black'),\n",
       " Row(value='RodFord OCEANOS Stage II Overhead Rod RFOB60-4 AHI - PE#4 (1 Sec.)'),\n",
       " Row(value='Giant killing 29'),\n",
       " Row(value=''),\n",
       " Row(value='New Stock sepatu kulit gio feruji sepatu kerja'),\n",
       " Row(value='Best Seller Kemeja Polos Pria Lengan Panjang Merah Marun Cpmm'),\n",
       " Row(value='kemeja pria bigsize  4L (Best Seller!)')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_rdd = product_names_df.rdd\n",
    "product_names_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='kemeja pria bigsize  4L (Best Seller!)'),\n",
       " Row(value='Sepaket Tas Dompet Sepatu Jam Kacamata Kalung'),\n",
       " Row(value='RodFord OCEANOS Stage II Overhead Rod RFOB60-4 AHI - PE#4 (1 Sec.)'),\n",
       " Row(value='Ready Stock Kulot Cantik'),\n",
       " Row(value='New Stock sepatu kulit gio feruji sepatu kerja'),\n",
       " Row(value='Giant killing 29'),\n",
       " Row(value='DAILY LIFE OF SCHOLAR SHINJIRO KATSURAGI 01: LC-MeruyaBookStore'),\n",
       " Row(value='Best Seller Kemeja Polos Pria Lengan Panjang Merah Marun Cpmm'),\n",
       " Row(value='Baterai HP Pavilion DV3 2000 Compaq Presario CQ35   Black'),\n",
       " Row(value='BARU.. Motorola TLKR T80  GO ADVENTURE  WALKIE TALKIES- GARANSI RESMI 1 TAHUN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_rdd.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(words):\n",
    "    return text_to_word_sequence(words['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_product_names_rdd = \\\n",
    "    product_names_rdd.flatMap(lambda product_name: tokenize(product_name))\n",
    "tokenized_product_names_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walkie',\n",
       " 'tlkr',\n",
       " 'tas',\n",
       " 'talkies',\n",
       " 'tahun',\n",
       " 't80',\n",
       " 'stock',\n",
       " 'stock',\n",
       " 'stage',\n",
       " 'shinjiro',\n",
       " 'sepatu',\n",
       " 'sepatu',\n",
       " 'sepatu',\n",
       " 'sepaket',\n",
       " 'seller',\n",
       " 'seller',\n",
       " 'sec',\n",
       " 'scholar',\n",
       " 'rodford',\n",
       " 'rod']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_product_names_rdd.top(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_product_names_rdd = \\\n",
    "    tokenized_product_names_rdd.map(lambda word: (word, 1)) \\\n",
    "                               .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[15] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_product_names_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('walkie', 1),\n",
       " ('tlkr', 1),\n",
       " ('tas', 1),\n",
       " ('talkies', 1),\n",
       " ('tahun', 1),\n",
       " ('t80', 1),\n",
       " ('stock', 2),\n",
       " ('stage', 1),\n",
       " ('shinjiro', 1),\n",
       " ('sepatu', 3),\n",
       " ('sepaket', 1),\n",
       " ('seller', 2),\n",
       " ('sec', 1),\n",
       " ('scholar', 1),\n",
       " ('rodford', 1),\n",
       " ('rod', 1),\n",
       " ('rfob60', 1),\n",
       " ('resmi', 1),\n",
       " ('ready', 1),\n",
       " ('pria', 2)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_product_names_rdd.top(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the output. __Note:__ Don't forget to delete existing `product_names_word_count_nb.orc` directory in `data/product_names_sample`. Following Spark implementation does not overwrite existing data but it will throw error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_product_names_df = spark.createDataFrame(word_count_product_names_rdd)\n",
    "word_count_product_names_df.write.save(product_names_word_count_nb_orc_filename, \\\n",
    "                                       format=\"orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_count_product_names_df = spark.read.orc(product_names_word_count_nb_orc_filename)\n",
    "new_word_count_product_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='daily', _2=1),\n",
       " Row(_1='life', _2=1),\n",
       " Row(_1='scholar', _2=1),\n",
       " Row(_1='shinjiro', _2=1),\n",
       " Row(_1='katsuragi', _2=1),\n",
       " Row(_1='01', _2=1),\n",
       " Row(_1='meruyabookstore', _2=1),\n",
       " Row(_1='stock', _2=2),\n",
       " Row(_1='tas', _2=1),\n",
       " Row(_1='sepatu', _2=3),\n",
       " Row(_1='kacamata', _2=1),\n",
       " Row(_1='kalung', _2=1),\n",
       " Row(_1='baterai', _2=1),\n",
       " Row(_1='pavilion', _2=1),\n",
       " Row(_1='dv3', _2=1),\n",
       " Row(_1='2000', _2=1),\n",
       " Row(_1='cq35', _2=1),\n",
       " Row(_1='black', _2=1),\n",
       " Row(_1='rodford', _2=1),\n",
       " Row(_1='ii', _2=1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_count_product_names_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Word Count using Spark Submit (SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bukalapak-core-ai.big-data-3v.volume-spark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bukalapak-core-ai.big-data-3v.volume-spark.py\n",
    "# Copyright (c) 2018 PT Bukalapak.com\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark\"\n",
    "\n",
    "\n",
    "def tokenize(words):\n",
    "    from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "    return text_to_word_sequence(words['value'])\n",
    "\n",
    "\n",
    "def main(spark):\n",
    "    # Input\n",
    "    product_names_text_filename = \\\n",
    "        \"file:/home/jovyan/work/\" + \\\n",
    "        \"data/product_names_sample/\" + \\\n",
    "        \"product_names.rdd\"\n",
    "    # Output\n",
    "    product_names_word_count_ss_orc_filename = \\\n",
    "        \"file:/home/jovyan/work/\" + \\\n",
    "        \"data/product_names_sample/\" + \\\n",
    "        \"product_names_word_count_ss.orc\"\n",
    "    # Read input\n",
    "    product_names_df = spark.read.text(product_names_text_filename)\n",
    "    product_names_rdd = product_names_df.rdd\n",
    "    # Perform tokenization and word count\n",
    "    tokenized_product_names_rdd = \\\n",
    "        product_names_rdd.flatMap(lambda product_name: tokenize(product_name))\n",
    "    word_count_product_names_rdd = \\\n",
    "        tokenized_product_names_rdd.map(lambda word: (word, 1)) \\\n",
    "                                   .reduceByKey(lambda a, b: a + b)\n",
    "    # Write output\n",
    "    word_count_product_names_df = spark.createDataFrame(word_count_product_names_rdd)\n",
    "    word_count_product_names_df.write.save(product_names_word_count_ss_orc_filename, \\\n",
    "                                           format=\"orc\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(APP_NAME) \\\n",
    "        .getOrCreate()\n",
    "    main(spark)\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Don't forget to delete existing `product_names_word_count_ss.orc` directory in `data/product_names_sample`. Following Spark implementation does not overwrite existing data but it will throw error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/10/18 17:34:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "19/10/18 17:34:52 INFO SparkContext: Running Spark version 2.4.4\n",
      "19/10/18 17:34:52 INFO SparkContext: Submitted application: bukalapak-core-ai.big-data-3v.volume-spark\n",
      "19/10/18 17:34:52 INFO SecurityManager: Changing view acls to: jovyan\n",
      "19/10/18 17:34:52 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "19/10/18 17:34:52 INFO SecurityManager: Changing view acls groups to: \n",
      "19/10/18 17:34:52 INFO SecurityManager: Changing modify acls groups to: \n",
      "19/10/18 17:34:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "19/10/18 17:34:52 INFO Utils: Successfully started service 'sparkDriver' on port 41903.\n",
      "19/10/18 17:34:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "19/10/18 17:34:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "19/10/18 17:34:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/10/18 17:34:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/10/18 17:34:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a193a978-1843-4b7a-8096-1264bc8ecceb\n",
      "19/10/18 17:34:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "19/10/18 17:34:52 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "19/10/18 17:34:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "19/10/18 17:34:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://25dd8860074f:4040\n",
      "19/10/18 17:34:52 INFO Executor: Starting executor ID driver on host localhost\n",
      "19/10/18 17:34:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37059.\n",
      "19/10/18 17:34:52 INFO NettyBlockTransferService: Server created on 25dd8860074f:37059\n",
      "19/10/18 17:34:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/10/18 17:34:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 25dd8860074f, 37059, None)\n",
      "19/10/18 17:34:52 INFO BlockManagerMasterEndpoint: Registering block manager 25dd8860074f:37059 with 366.3 MB RAM, BlockManagerId(driver, 25dd8860074f, 37059, None)\n",
      "19/10/18 17:34:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 25dd8860074f, 37059, None)\n",
      "19/10/18 17:34:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 25dd8860074f, 37059, None)\n",
      "19/10/18 17:34:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/work/spark-warehouse/').\n",
      "19/10/18 17:34:52 INFO SharedState: Warehouse path is 'file:/home/jovyan/work/spark-warehouse/'.\n",
      "19/10/18 17:34:53 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "19/10/18 17:34:54 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/10/18 17:34:54 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "19/10/18 17:34:54 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "19/10/18 17:34:54 INFO FileSourceScanExec: Pushed Filters: \n",
      "19/10/18 17:34:54 INFO CodeGenerator: Code generated in 111.388931 ms\n",
      "19/10/18 17:34:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 283.7 KB, free 366.0 MB)\n",
      "19/10/18 17:34:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 366.0 MB)\n",
      "19/10/18 17:34:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 25dd8860074f:37059 (size: 23.3 KB, free: 366.3 MB)\n",
      "19/10/18 17:34:54 INFO SparkContext: Created broadcast 0 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "19/10/18 17:34:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/10/18 17:34:55 INFO SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
      "19/10/18 17:34:55 INFO DAGScheduler: Registering RDD 5 (reduceByKey at /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py:51)\n",
      "19/10/18 17:34:55 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "19/10/18 17:34:55 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:153)\n",
      "19/10/18 17:34:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "19/10/18 17:34:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "19/10/18 17:34:55 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[5] at reduceByKey at /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py:51), which has no missing parents\n",
      "19/10/18 17:34:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 15.3 KB, free 366.0 MB)\n",
      "19/10/18 17:34:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.3 KB, free 366.0 MB)\n",
      "19/10/18 17:34:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 25dd8860074f:37059 (size: 9.3 KB, free: 366.3 MB)\n",
      "19/10/18 17:34:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\n",
      "19/10/18 17:34:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[5] at reduceByKey at /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py:51) (first 15 tasks are for partitions Vector(0, 1))\n",
      "19/10/18 17:34:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "19/10/18 17:34:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8330 bytes)\n",
      "19/10/18 17:34:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 8330 bytes)\n",
      "19/10/18 17:34:55 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "19/10/18 17:34:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "19/10/18 17:34:55 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/product_names_sample/product_names.rdd/part-00000-761349cc-6ec5-4482-bb8d-f328d24686b3-c000.txt, range: 0-260, partition values: [empty row]\n",
      "19/10/18 17:34:55 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/product_names_sample/product_names.rdd/part-00000-761349cc-6ec5-4482-bb8d-f328d24686b3-c001.txt, range: 0-244, partition values: [empty row]\n",
      "19/10/18 17:34:55 INFO CodeGenerator: Code generated in 11.06489 ms\n",
      "19/10/18 17:34:57 INFO PythonRunner: Times: total = 1851, boot = 382, init = 66, finish = 1403\n",
      "19/10/18 17:34:57 INFO PythonRunner: Times: total = 1896, boot = 385, init = 63, finish = 1448\n",
      "19/10/18 17:34:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2043 bytes result sent to driver\n",
      "19/10/18 17:34:57 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2043 bytes result sent to driver\n",
      "19/10/18 17:34:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2103 ms on localhost (executor driver) (1/2)\n",
      "19/10/18 17:34:57 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2096 ms on localhost (executor driver) (2/2)\n",
      "19/10/18 17:34:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "19/10/18 17:34:57 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50103\n",
      "19/10/18 17:34:57 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py:51) finished in 2.171 s\n",
      "19/10/18 17:34:57 INFO DAGScheduler: looking for newly runnable stages\n",
      "19/10/18 17:34:57 INFO DAGScheduler: running: Set()\n",
      "19/10/18 17:34:57 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "19/10/18 17:34:57 INFO DAGScheduler: failed: Set()\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[8] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "19/10/18 17:34:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.3 KB, free 366.0 MB)\n",
      "19/10/18 17:34:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.4 KB, free 366.0 MB)\n",
      "19/10/18 17:34:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 25dd8860074f:37059 (size: 5.4 KB, free: 366.3 MB)\n",
      "19/10/18 17:34:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[8] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "19/10/18 17:34:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "19/10/18 17:34:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7662 bytes)\n",
      "19/10/18 17:34:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
      "19/10/18 17:34:57 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/10/18 17:34:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "19/10/18 17:34:57 INFO PythonRunner: Times: total = 8, boot = -182, init = 190, finish = 0\n",
      "19/10/18 17:34:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1774 bytes result sent to driver\n",
      "19/10/18 17:34:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 39 ms on localhost (executor driver) (1/1)\n",
      "19/10/18 17:34:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "19/10/18 17:34:57 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:153) finished in 0.048 s\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:153, took 2.259463 s\n",
      "19/10/18 17:34:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/10/18 17:34:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/10/18 17:34:57 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Got job 1 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Missing parents: List()\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "19/10/18 17:34:57 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 153.8 KB, free 365.8 MB)\n",
      "19/10/18 17:34:57 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 57.5 KB, free 365.8 MB)\n",
      "19/10/18 17:34:57 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 25dd8860074f:37059 (size: 57.5 KB, free: 366.2 MB)\n",
      "19/10/18 17:34:57 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "19/10/18 17:34:57 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks\n",
      "19/10/18 17:34:57 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 7662 bytes)\n",
      "19/10/18 17:34:57 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, executor driver, partition 1, ANY, 7662 bytes)\n",
      "19/10/18 17:34:57 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)\n",
      "19/10/18 17:34:57 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "19/10/18 17:34:57 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/10/18 17:34:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/10/18 17:34:57 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/10/18 17:34:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/10/18 17:34:57 INFO CodeGenerator: Code generated in 11.509489 ms\n",
      "19/10/18 17:34:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/10/18 17:34:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/10/18 17:34:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/10/18 17:34:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/10/18 17:34:57 INFO MemoryManagerImpl: orc.rows.between.memory.checks=5000\n",
      "19/10/18 17:34:57 INFO MemoryManagerImpl: orc.rows.between.memory.checks=5000\n",
      "19/10/18 17:34:57 INFO PhysicalFsWriter: ORC writer created for path: file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc/_temporary/0/_temporary/attempt_20191018173457_0003_m_000000_3/part-00000-4619cf9d-fb80-44aa-887c-35be3bef7139-c000.snappy.orc with stripeSize: 67108864 blockSize: 268435456 compression: SNAPPY bufferSize: 262144\n",
      "19/10/18 17:34:57 INFO PhysicalFsWriter: ORC writer created for path: file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc/_temporary/0/_temporary/attempt_20191018173457_0003_m_000001_4/part-00001-4619cf9d-fb80-44aa-887c-35be3bef7139-c000.snappy.orc with stripeSize: 67108864 blockSize: 268435456 compression: SNAPPY bufferSize: 262144\n",
      "19/10/18 17:34:57 INFO OrcCodecPool: Got brand-new codec SNAPPY\n",
      "19/10/18 17:34:57 INFO OrcCodecPool: Got brand-new codec SNAPPY\n",
      "19/10/18 17:34:57 INFO WriterImpl: ORC writer created for path: file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc/_temporary/0/_temporary/attempt_20191018173457_0003_m_000000_3/part-00000-4619cf9d-fb80-44aa-887c-35be3bef7139-c000.snappy.orc with stripeSize: 67108864 blockSize: 268435456 compression: SNAPPY bufferSize: 262144\n",
      "19/10/18 17:34:57 INFO WriterImpl: ORC writer created for path: file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc/_temporary/0/_temporary/attempt_20191018173457_0003_m_000001_4/part-00001-4619cf9d-fb80-44aa-887c-35be3bef7139-c000.snappy.orc with stripeSize: 67108864 blockSize: 268435456 compression: SNAPPY bufferSize: 262144\n",
      "19/10/18 17:34:57 INFO PythonRunner: Times: total = 44, boot = -180, init = 224, finish = 0\n",
      "19/10/18 17:34:57 INFO PythonRunner: Times: total = 2, boot = -365, init = 367, finish = 0\n",
      "19/10/18 17:34:57 INFO FileOutputCommitter: Saved output of task 'attempt_20191018173457_0003_m_000001_4' to file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc/_temporary/0/task_20191018173457_0003_m_000001\n",
      "19/10/18 17:34:57 INFO FileOutputCommitter: Saved output of task 'attempt_20191018173457_0003_m_000000_3' to file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc/_temporary/0/task_20191018173457_0003_m_000000\n",
      "19/10/18 17:34:57 INFO SparkHadoopMapRedUtil: attempt_20191018173457_0003_m_000000_3: Committed\n",
      "19/10/18 17:34:57 INFO SparkHadoopMapRedUtil: attempt_20191018173457_0003_m_000001_4: Committed\n",
      "19/10/18 17:34:57 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 2985 bytes result sent to driver\n",
      "19/10/18 17:34:57 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2985 bytes result sent to driver\n",
      "19/10/18 17:34:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 188 ms on localhost (executor driver) (1/2)\n",
      "19/10/18 17:34:57 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 188 ms on localhost (executor driver) (2/2)\n",
      "19/10/18 17:34:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "19/10/18 17:34:57 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 0.212 s\n",
      "19/10/18 17:34:57 INFO DAGScheduler: Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 0.217563 s\n",
      "19/10/18 17:34:57 INFO FileFormatWriter: Write Job e7930425-f814-4e4c-bd9a-2ff595e4b532 committed.\n",
      "19/10/18 17:34:57 INFO FileFormatWriter: Finished processing stats for write job e7930425-f814-4e4c-bd9a-2ff595e4b532.\n",
      "19/10/18 17:34:57 INFO SparkUI: Stopped Spark web UI at http://25dd8860074f:4040\n",
      "19/10/18 17:34:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "19/10/18 17:34:57 INFO MemoryStore: MemoryStore cleared\n",
      "19/10/18 17:34:57 INFO BlockManager: BlockManager stopped\n",
      "19/10/18 17:34:57 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "19/10/18 17:34:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "19/10/18 17:34:57 INFO SparkContext: Successfully stopped SparkContext\n",
      "19/10/18 17:34:58 INFO ShutdownHookManager: Shutdown hook called\n",
      "19/10/18 17:34:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-798d12e5-63f8-41b0-91de-0a30d273b73d\n",
      "19/10/18 17:34:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-58cb6ba2-b13a-42ca-9a2b-ad2bc539ff52\n",
      "19/10/18 17:34:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-798d12e5-63f8-41b0-91de-0a30d273b73d/pyspark-a317fdb3-a6b7-40de-a9f5-b307ccc7b1ba\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/spark/bin/spark-submit \\\n",
    "    --executor-memory 1g --executor-cores 1 --num-executors 2 \\\n",
    "    bukalapak-core-ai.big-data-3v.volume-spark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_word_count_ss_orc_filename = \\\n",
    "    \"file:/home/jovyan/work/\" + \\\n",
    "    \"data/product_names_sample/\" + \\\n",
    "    \"product_names_word_count_ss.orc\"\n",
    "product_names_word_count_ss_orc_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_count_product_names_df = spark.read.orc(product_names_word_count_ss_orc_filename)\n",
    "new_word_count_product_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='daily', _2=1),\n",
       " Row(_1='life', _2=1),\n",
       " Row(_1='scholar', _2=1),\n",
       " Row(_1='shinjiro', _2=1),\n",
       " Row(_1='katsuragi', _2=1),\n",
       " Row(_1='01', _2=1),\n",
       " Row(_1='meruyabookstore', _2=1),\n",
       " Row(_1='stock', _2=2),\n",
       " Row(_1='tas', _2=1),\n",
       " Row(_1='sepatu', _2=3),\n",
       " Row(_1='kacamata', _2=1),\n",
       " Row(_1='kalung', _2=1),\n",
       " Row(_1='baterai', _2=1),\n",
       " Row(_1='pavilion', _2=1),\n",
       " Row(_1='dv3', _2=1),\n",
       " Row(_1='2000', _2=1),\n",
       " Row(_1='cq35', _2=1),\n",
       " Row(_1='black', _2=1),\n",
       " Row(_1='rodford', _2=1),\n",
       " Row(_1='ii', _2=1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_count_product_names_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME=\"Ubuntu\"\n",
      "VERSION=\"18.04.2 LTS (Bionic Beaver)\"\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "PRETTY_NAME=\"Ubuntu 18.04.2 LTS\"\n",
      "VERSION_ID=\"18.04\"\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "VERSION_CODENAME=bionic\n",
      "UBUNTU_CODENAME=bionic\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.8.1\n",
      "alembic==1.0.11\n",
      "asn1crypto==0.24.0\n",
      "astor==0.8.0\n",
      "async-generator==1.10\n",
      "attrs==19.1.0\n",
      "backcall==0.1.0\n",
      "beautifulsoup4==4.7.1\n",
      "bleach==3.1.0\n",
      "blinker==1.4\n",
      "bokeh==1.0.4\n",
      "certifi==2019.9.11\n",
      "certipy==0.1.3\n",
      "cffi==1.12.3\n",
      "chardet==3.0.4\n",
      "Click==7.0\n",
      "cloudpickle==0.8.1\n",
      "conda==4.7.10\n",
      "conda-package-handling==1.3.11\n",
      "cryptography==2.7\n",
      "cycler==0.10.0\n",
      "Cython==0.29.13\n",
      "cytoolz==0.10.0\n",
      "dask==1.1.5\n",
      "decorator==4.4.0\n",
      "defusedxml==0.5.0\n",
      "dill==0.2.9\n",
      "distributed==1.28.1\n",
      "entrypoints==0.3\n",
      "fastcache==1.1.0\n",
      "gast==0.2.2\n",
      "gmpy2==2.1.0b1\n",
      "google-pasta==0.1.7\n",
      "grpcio==1.24.1\n",
      "h5py==2.9.0\n",
      "heapdict==1.0.0\n",
      "idna==2.8\n",
      "imageio==2.5.0\n",
      "ipykernel==5.1.1\n",
      "ipython==7.7.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.0\n",
      "jedi==0.14.1\n",
      "Jinja2==2.10.1\n",
      "json5==0.8.5\n",
      "jsonschema==3.0.1\n",
      "jupyter-client==5.3.1\n",
      "jupyter-core==4.4.0\n",
      "jupyterhub==1.0.0\n",
      "jupyterlab==1.0.4\n",
      "jupyterlab-server==1.0.0\n",
      "Keras-Applications==1.0.8\n",
      "Keras-Preprocessing==1.1.0\n",
      "kiwisolver==1.1.0\n",
      "libarchive-c==2.8\n",
      "llvmlite==0.27.1\n",
      "locket==0.2.0\n",
      "Mako==1.0.10\n",
      "Markdown==3.1.1\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.1.1\n",
      "mistune==0.8.4\n",
      "mpmath==1.1.0\n",
      "msgpack==0.6.1\n",
      "nbconvert==5.5.0\n",
      "nbformat==4.4.0\n",
      "networkx==2.3\n",
      "notebook==6.0.0\n",
      "numba==0.42.1\n",
      "numexpr==2.6.9\n",
      "numpy==1.17.3\n",
      "oauthlib==3.0.1\n",
      "olefile==0.46\n",
      "opt-einsum==3.1.0\n",
      "packaging==19.0\n",
      "pamela==1.0.0\n",
      "pandas==0.24.2\n",
      "pandocfilters==1.4.2\n",
      "parso==0.5.1\n",
      "partd==1.0.0\n",
      "patsy==0.5.1\n",
      "pexpect==4.7.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==6.1.0\n",
      "prometheus-client==0.7.1\n",
      "prompt-toolkit==2.0.9\n",
      "protobuf==3.7.1\n",
      "psutil==5.6.3\n",
      "ptyprocess==0.6.0\n",
      "pyarrow==0.13.0\n",
      "pycosat==0.6.3\n",
      "pycparser==2.19\n",
      "pycurl==7.43.0.2\n",
      "Pygments==2.4.2\n",
      "PyJWT==1.7.1\n",
      "pyOpenSSL==19.0.0\n",
      "pyparsing==2.4.1.1\n",
      "pyrsistent==0.15.3\n",
      "PySocks==1.7.0\n",
      "pyspark==2.4.4\n",
      "python-dateutil==2.8.0\n",
      "python-editor==1.0.4\n",
      "pytz==2019.1\n",
      "PyWavelets==1.0.3\n",
      "PyYAML==5.1.1\n",
      "pyzmq==18.0.2\n",
      "requests==2.22.0\n",
      "ruamel-yaml==0.15.71\n",
      "scikit-image==0.14.3\n",
      "scikit-learn==0.20.3\n",
      "scipy==1.2.1\n",
      "seaborn==0.9.0\n",
      "Send2Trash==1.5.0\n",
      "six==1.12.0\n",
      "sortedcontainers==2.1.0\n",
      "soupsieve==1.9.2\n",
      "SQLAlchemy==1.3.6\n",
      "statsmodels==0.9.0\n",
      "sympy==1.3\n",
      "tblib==1.4.0\n",
      "tensorboard==1.15.0\n",
      "tensorflow==1.15.0\n",
      "tensorflow-estimator==1.15.1\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.2\n",
      "testpath==0.4.2\n",
      "toolz==0.10.0\n",
      "tornado==6.0.3\n",
      "tqdm==4.32.2\n",
      "traitlets==4.3.2\n",
      "urllib3==1.25.3\n",
      "vincent==0.4.4\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.16.0\n",
      "widgetsnbextension==3.5.0\n",
      "wrapt==1.11.2\n",
      "xlrd==1.2.0\n",
      "zict==1.0.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /opt/conda:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main    defaults\n",
      "absl-py                   0.8.1                    pypi_0    pypi\n",
      "alembic                   1.0.11                     py_0    conda-forge\n",
      "arrow-cpp                 0.13.0           py37h246e31e_6    conda-forge\n",
      "asn1crypto                0.24.0                py37_1003    conda-forge\n",
      "astor                     0.8.0                    pypi_0    pypi\n",
      "async_generator           1.10                       py_0    conda-forge\n",
      "attrs                     19.1.0                     py_0    conda-forge\n",
      "backcall                  0.1.0                      py_0    conda-forge\n",
      "beautifulsoup4            4.7.1                 py37_1001    conda-forge\n",
      "blas                      2.10                   openblas    conda-forge\n",
      "bleach                    3.1.0                      py_0    conda-forge\n",
      "blinker                   1.4                        py_1    conda-forge\n",
      "bokeh                     1.0.4                 py37_1000    conda-forge\n",
      "boost-cpp                 1.70.0               h8e57a91_2    conda-forge\n",
      "brotli                    1.0.7             he1b5a44_1000    conda-forge\n",
      "bzip2                     1.0.8                h516909a_0    conda-forge\n",
      "ca-certificates           2019.9.11            hecc5488_0    conda-forge\n",
      "certifi                   2019.9.11                py37_0    conda-forge\n",
      "certipy                   0.1.3                      py_0    conda-forge\n",
      "cffi                      1.12.3           py37h8022711_0    conda-forge\n",
      "chardet                   3.0.4                 py37_1003    conda-forge\n",
      "click                     7.0                        py_0    conda-forge\n",
      "cloudpickle               0.8.1                      py_0    conda-forge\n",
      "conda                     4.7.10                   py37_0    conda-forge\n",
      "conda-package-handling    1.3.11                   py37_0    conda-forge\n",
      "configurable-http-proxy   4.1.0                  node11_1    conda-forge\n",
      "cryptography              2.7              py37h72c5cf5_0    conda-forge\n",
      "cycler                    0.10.0                     py_1    conda-forge\n",
      "cython                    0.29.13          py37he1b5a44_0    conda-forge\n",
      "cytoolz                   0.10.0           py37h516909a_0    conda-forge\n",
      "dask                      1.1.5                      py_0    conda-forge\n",
      "dask-core                 1.1.5                      py_0    conda-forge\n",
      "decorator                 4.4.0                      py_0    conda-forge\n",
      "defusedxml                0.5.0                      py_1    conda-forge\n",
      "dill                      0.2.9                    py37_0    conda-forge\n",
      "distributed               1.28.1                   py37_0    conda-forge\n",
      "double-conversion         3.1.5                he1b5a44_1    conda-forge\n",
      "entrypoints               0.3                   py37_1000    conda-forge\n",
      "fastcache                 1.1.0            py37h516909a_0    conda-forge\n",
      "freetype                  2.10.0               he983fc9_0    conda-forge\n",
      "gast                      0.2.2                    pypi_0    pypi\n",
      "gflags                    2.2.2             he1b5a44_1001    conda-forge\n",
      "glog                      0.4.0                he1b5a44_1    conda-forge\n",
      "gmp                       6.1.2             hf484d3e_1000    conda-forge\n",
      "gmpy2                     2.1.0b1          py37h04dde30_0    conda-forge\n",
      "google-pasta              0.1.7                    pypi_0    pypi\n",
      "grpcio                    1.24.1                   pypi_0    pypi\n",
      "h5py                      2.9.0           nompi_py37hf008753_1102    conda-forge\n",
      "hdf5                      1.10.4          nompi_h3c11f04_1106    conda-forge\n",
      "heapdict                  1.0.0                 py37_1000    conda-forge\n",
      "icu                       64.2                 he1b5a44_0    conda-forge\n",
      "idna                      2.8                   py37_1000    conda-forge\n",
      "imageio                   2.5.0                    py37_0    conda-forge\n",
      "ipykernel                 5.1.1            py37h5ca1d4c_0    conda-forge\n",
      "ipython                   7.7.0            py37h5ca1d4c_0    conda-forge\n",
      "ipython_genutils          0.2.0                      py_1    conda-forge\n",
      "ipywidgets                7.5.0                      py_0    conda-forge\n",
      "jedi                      0.14.1                   py37_0    conda-forge\n",
      "jinja2                    2.10.1                     py_0    conda-forge\n",
      "jpeg                      9c                h14c3975_1001    conda-forge\n",
      "json5                     0.8.5                      py_0    conda-forge\n",
      "jsonschema                3.0.1                    py37_0    conda-forge\n",
      "jupyter_client            5.3.1                      py_0    conda-forge\n",
      "jupyter_core              4.4.0                      py_0    conda-forge\n",
      "jupyterhub                1.0.0                    py37_0    conda-forge\n",
      "jupyterlab                1.0.4                    py37_0    conda-forge\n",
      "jupyterlab_server         1.0.0                      py_1    conda-forge\n",
      "keras-applications        1.0.8                    pypi_0    pypi\n",
      "keras-preprocessing       1.1.0                    pypi_0    pypi\n",
      "kiwisolver                1.1.0            py37hc9558a2_0    conda-forge\n",
      "krb5                      1.16.3            h05b26f9_1001    conda-forge\n",
      "libarchive                3.3.3             hb44662c_1005    conda-forge\n",
      "libblas                   3.8.0               10_openblas    conda-forge\n",
      "libcblas                  3.8.0               10_openblas    conda-forge\n",
      "libcurl                   7.65.3               hda55be3_0    conda-forge\n",
      "libedit                   3.1.20170329      hf8c457e_1001    conda-forge\n",
      "libevent                  2.1.10               h72c5cf5_0    conda-forge\n",
      "libffi                    3.2.1             he1b5a44_1006    conda-forge\n",
      "libgcc-ng                 9.1.0                hdf63c60_0    defaults\n",
      "libgfortran-ng            7.3.0                hdf63c60_0    defaults\n",
      "libiconv                  1.15              h516909a_1005    conda-forge\n",
      "liblapack                 3.8.0               10_openblas    conda-forge\n",
      "liblapacke                3.8.0               10_openblas    conda-forge\n",
      "libopenblas               0.3.6                h6e990d7_5    conda-forge\n",
      "libpng                    1.6.37               hed695b0_0    conda-forge\n",
      "libprotobuf               3.7.1                h8b12597_0    conda-forge\n",
      "libsodium                 1.0.17               h516909a_0    conda-forge\n",
      "libssh2                   1.8.2                h22169c7_2    conda-forge\n",
      "libstdcxx-ng              9.1.0                hdf63c60_0    defaults\n",
      "libtiff                   4.0.10            h57b8799_1003    conda-forge\n",
      "libxml2                   2.9.9                hee79883_2    conda-forge\n",
      "llvmlite                  0.27.1           py37hdbcaa40_0    conda-forge\n",
      "locket                    0.2.0                      py_2    conda-forge\n",
      "lz4-c                     1.8.3             he1b5a44_1001    conda-forge\n",
      "lzo                       2.10              h14c3975_1000    conda-forge\n",
      "mako                      1.0.10                     py_0    conda-forge\n",
      "markdown                  3.1.1                    pypi_0    pypi\n",
      "markupsafe                1.1.1            py37h14c3975_0    conda-forge\n",
      "matplotlib-base           3.1.1            py37he7580a8_1    conda-forge\n",
      "mistune                   0.8.4           py37h14c3975_1000    conda-forge\n",
      "mpc                       1.1.0             hb20f59a_1006    conda-forge\n",
      "mpfr                      4.0.2                ha14ba45_0    conda-forge\n",
      "mpmath                    1.1.0                      py_0    conda-forge\n",
      "msgpack-python            0.6.1            py37h6bb024c_0    conda-forge\n",
      "nbconvert                 5.5.0                      py_0    conda-forge\n",
      "nbformat                  4.4.0                      py_1    conda-forge\n",
      "ncurses                   6.1               hf484d3e_1002    conda-forge\n",
      "networkx                  2.3                        py_0    conda-forge\n",
      "nodejs                    11.14.0              he1b5a44_1    conda-forge\n",
      "notebook                  6.0.0                    py37_0    conda-forge\n",
      "numba                     0.42.1           py37hf484d3e_0    conda-forge\n",
      "numexpr                   2.6.9           py37h637b7d7_1000    conda-forge\n",
      "numpy                     1.17.3                   pypi_0    pypi\n",
      "oauthlib                  3.0.1                      py_0    conda-forge\n",
      "olefile                   0.46                       py_0    conda-forge\n",
      "openblas                  0.3.6                h6e990d7_5    conda-forge\n",
      "openssl                   1.1.1c               h516909a_0    conda-forge\n",
      "opt-einsum                3.1.0                    pypi_0    pypi\n",
      "packaging                 19.0                       py_0    conda-forge\n",
      "pamela                    1.0.0                      py_0    conda-forge\n",
      "pandas                    0.24.2           py37hb3f55d8_0    conda-forge\n",
      "pandoc                    2.7.3                         0    conda-forge\n",
      "pandocfilters             1.4.2                      py_1    conda-forge\n",
      "parquet-cpp               1.5.1                         2    conda-forge\n",
      "parso                     0.5.1                      py_0    conda-forge\n",
      "partd                     1.0.0                      py_0    conda-forge\n",
      "patsy                     0.5.1                      py_0    conda-forge\n",
      "pexpect                   4.7.0                    py37_0    conda-forge\n",
      "pickleshare               0.7.5                 py37_1000    conda-forge\n",
      "pillow                    6.1.0            py37he7afcd5_0    conda-forge\n",
      "pip                       19.2.1                   py37_0    conda-forge\n",
      "prometheus_client         0.7.1                      py_0    conda-forge\n",
      "prompt_toolkit            2.0.9                      py_0    conda-forge\n",
      "protobuf                  3.7.1            py37he1b5a44_0    conda-forge\n",
      "psutil                    5.6.3            py37h516909a_0    conda-forge\n",
      "ptyprocess                0.6.0                   py_1001    conda-forge\n",
      "pyarrow                   0.13.0           py37h8b68381_2    conda-forge\n",
      "pycosat                   0.6.3           py37h14c3975_1001    conda-forge\n",
      "pycparser                 2.19                     py37_1    conda-forge\n",
      "pycurl                    7.43.0.2         py37h16ce93b_1    conda-forge\n",
      "pygments                  2.4.2                      py_0    conda-forge\n",
      "pyjwt                     1.7.1                      py_0    conda-forge\n",
      "pyopenssl                 19.0.0                   py37_0    conda-forge\n",
      "pyparsing                 2.4.1.1                    py_0    conda-forge\n",
      "pyrsistent                0.15.3           py37h516909a_0    conda-forge\n",
      "pysocks                   1.7.0                    py37_0    conda-forge\n",
      "python                    3.7.3                h33d41f4_1    conda-forge\n",
      "python-dateutil           2.8.0                      py_0    conda-forge\n",
      "python-editor             1.0.4                      py_0    conda-forge\n",
      "python-libarchive-c       2.8                   py37_1004    conda-forge\n",
      "pytz                      2019.1                     py_0    conda-forge\n",
      "pywavelets                1.0.3            py37hd352d35_1    conda-forge\n",
      "pyyaml                    5.1.1            py37h516909a_0    conda-forge\n",
      "pyzmq                     18.0.2           py37h1768529_2    conda-forge\n",
      "re2                       2019.09.01           he1b5a44_0    conda-forge\n",
      "readline                  8.0                  hf8c457e_0    conda-forge\n",
      "requests                  2.22.0                   py37_1    conda-forge\n",
      "ruamel_yaml               0.15.71         py37h14c3975_1000    conda-forge\n",
      "scikit-image              0.14.3           py37hb3f55d8_0    conda-forge\n",
      "scikit-learn              0.20.3           py37ha8026db_1    conda-forge\n",
      "scipy                     1.2.1            py37h09a28d5_1    conda-forge\n",
      "seaborn                   0.9.0                      py_1    conda-forge\n",
      "send2trash                1.5.0                      py_0    conda-forge\n",
      "setuptools                41.0.1                   py37_0    conda-forge\n",
      "six                       1.12.0                py37_1000    conda-forge\n",
      "snappy                    1.1.7             he1b5a44_1002    conda-forge\n",
      "sortedcontainers          2.1.0                      py_0    conda-forge\n",
      "soupsieve                 1.9.2                    py37_0    conda-forge\n",
      "sqlalchemy                1.3.6            py37h516909a_0    conda-forge\n",
      "sqlite                    3.29.0               hcee41ef_0    conda-forge\n",
      "statsmodels               0.9.0           py37h3010b51_1000    conda-forge\n",
      "sympy                     1.3                   py37_1000    conda-forge\n",
      "tblib                     1.4.0                      py_0    conda-forge\n",
      "tensorboard               1.15.0                   pypi_0    pypi\n",
      "tensorflow                1.15.0                   pypi_0    pypi\n",
      "tensorflow-estimator      1.15.1                   pypi_0    pypi\n",
      "termcolor                 1.1.0                    pypi_0    pypi\n",
      "terminado                 0.8.2                    py37_0    conda-forge\n",
      "testpath                  0.4.2                   py_1001    conda-forge\n",
      "thrift-cpp                0.12.0            hf3afdfd_1004    conda-forge\n",
      "tini                      0.18.0            h14c3975_1001    conda-forge\n",
      "tk                        8.6.9             hed695b0_1002    conda-forge\n",
      "toolz                     0.10.0                     py_0    conda-forge\n",
      "tornado                   6.0.3            py37h516909a_0    conda-forge\n",
      "tqdm                      4.32.2                     py_0    conda-forge\n",
      "traitlets                 4.3.2                 py37_1000    conda-forge\n",
      "urllib3                   1.25.3                   py37_0    conda-forge\n",
      "vincent                   0.4.4                      py_1    conda-forge\n",
      "wcwidth                   0.1.7                      py_1    conda-forge\n",
      "webencodings              0.5.1                      py_1    conda-forge\n",
      "werkzeug                  0.16.0                   pypi_0    pypi\n",
      "wheel                     0.33.4                   py37_0    conda-forge\n",
      "widgetsnbextension        3.5.0                    py37_0    conda-forge\n",
      "wrapt                     1.11.2                   pypi_0    pypi\n",
      "xlrd                      1.2.0                      py_0    conda-forge\n",
      "xz                        5.2.4             h14c3975_1001    conda-forge\n",
      "yaml                      0.1.7             h14c3975_1001    conda-forge\n",
      "zeromq                    4.3.2                he1b5a44_2    conda-forge\n",
      "zict                      1.0.0                      py_0    conda-forge\n",
      "zlib                      1.2.11            h516909a_1005    conda-forge\n",
      "zstd                      1.4.0                h3b9ef0a_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
