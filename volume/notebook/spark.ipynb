{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Volume with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Apache Spark to perform word count on product names after tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 PT Bukalapak.com\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.3 |Anaconda, Inc.| (default, Nov  9 2017, 00:19:18) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(\"Python %s\" % sys.version)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(\"PySpark %s\" % pyspark.__version__)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform 1.0.8\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(\"platform %s\" % platform.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS Linux-4.15.0-34-generic-x86_64-with-debian-buster-sid\n"
     ]
    }
   ],
   "source": [
    "print(\"OS\", platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow %s\" % tf.__version__)\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.11.8, OpenJDK 64-Bit Server VM, 1.8.0_181\n",
      "Branch \n",
      "Compiled by user vanzin on 2018-06-01T20:37:04Z\n",
      "Revision \n",
      "Url \n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/spark/bin/spark-submit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Word Count using Notebook (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c7831edbaee2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>bukalapak-core-ai.big-data-3v.volume-spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=bukalapak-core-ai.big-data-3v.volume-spark>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and output URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/home/jovyan/work/data/product_names_sample/product_names.rdd'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_text_filename = \\\n",
    "    \"file:/home/jovyan/work/\" + \\\n",
    "    \"data/product_names_sample/\" + \\\n",
    "    \"product_names.rdd\"\n",
    "product_names_text_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/home/jovyan/work/data/product_names_sample/product_names_word_count_nb.orc'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_word_count_nb_orc_filename = \\\n",
    "    \"file:/home/jovyan/work/\" + \\\n",
    "    \"data/product_names_sample/\" + \\\n",
    "    \"product_names_word_count_nb.orc\"\n",
    "product_names_word_count_nb_orc_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_df = spark.read.text(product_names_text_filename)\n",
    "product_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='DAILY LIFE OF SCHOLAR SHINJIRO KATSURAGI 01: LC-MeruyaBookStore'),\n",
       " Row(value='Ready Stock Kulot Cantik'),\n",
       " Row(value='Sepaket Tas Dompet Sepatu Jam Kacamata Kalung'),\n",
       " Row(value='Baterai HP Pavilion DV3 2000 Compaq Presario CQ35   Black'),\n",
       " Row(value='RodFord OCEANOS Stage II Overhead Rod RFOB60-4 AHI - PE#4 (1 Sec.)'),\n",
       " Row(value='Giant killing 29'),\n",
       " Row(value=''),\n",
       " Row(value='New Stock sepatu kulit gio feruji sepatu kerja'),\n",
       " Row(value='Best Seller Kemeja Polos Pria Lengan Panjang Merah Marun Cpmm'),\n",
       " Row(value='kemeja pria bigsize  4L (Best Seller!)')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_rdd = product_names_df.rdd\n",
    "product_names_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='kemeja pria bigsize  4L (Best Seller!)'),\n",
       " Row(value='Sepaket Tas Dompet Sepatu Jam Kacamata Kalung'),\n",
       " Row(value='RodFord OCEANOS Stage II Overhead Rod RFOB60-4 AHI - PE#4 (1 Sec.)'),\n",
       " Row(value='Ready Stock Kulot Cantik'),\n",
       " Row(value='New Stock sepatu kulit gio feruji sepatu kerja'),\n",
       " Row(value='Giant killing 29'),\n",
       " Row(value='DAILY LIFE OF SCHOLAR SHINJIRO KATSURAGI 01: LC-MeruyaBookStore'),\n",
       " Row(value='Best Seller Kemeja Polos Pria Lengan Panjang Merah Marun Cpmm'),\n",
       " Row(value='Baterai HP Pavilion DV3 2000 Compaq Presario CQ35   Black'),\n",
       " Row(value='BARU.. Motorola TLKR T80  GO ADVENTURE  WALKIE TALKIES- GARANSI RESMI 1 TAHUN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_rdd.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(words):\n",
    "    return text_to_word_sequence(words['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_product_names_rdd = \\\n",
    "    product_names_rdd.flatMap(lambda product_name: tokenize(product_name))\n",
    "tokenized_product_names_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walkie',\n",
       " 'tlkr',\n",
       " 'tas',\n",
       " 'talkies',\n",
       " 'tahun',\n",
       " 't80',\n",
       " 'stock',\n",
       " 'stock',\n",
       " 'stage',\n",
       " 'shinjiro',\n",
       " 'sepatu',\n",
       " 'sepatu',\n",
       " 'sepatu',\n",
       " 'sepaket',\n",
       " 'seller',\n",
       " 'seller',\n",
       " 'sec',\n",
       " 'scholar',\n",
       " 'rodford',\n",
       " 'rod']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_product_names_rdd.top(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_product_names_rdd = \\\n",
    "    tokenized_product_names_rdd.map(lambda word: (word, 1)) \\\n",
    "                               .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[15] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_product_names_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('walkie', 1),\n",
       " ('tlkr', 1),\n",
       " ('tas', 1),\n",
       " ('talkies', 1),\n",
       " ('tahun', 1),\n",
       " ('t80', 1),\n",
       " ('stock', 2),\n",
       " ('stage', 1),\n",
       " ('shinjiro', 1),\n",
       " ('sepatu', 3),\n",
       " ('sepaket', 1),\n",
       " ('seller', 2),\n",
       " ('sec', 1),\n",
       " ('scholar', 1),\n",
       " ('rodford', 1),\n",
       " ('rod', 1),\n",
       " ('rfob60', 1),\n",
       " ('resmi', 1),\n",
       " ('ready', 1),\n",
       " ('pria', 2)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_product_names_rdd.top(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the output. __Note:__ Don't forget to delete existing `product_names_word_count_nb.orc` directory in `data/product_names_sample`. Following Spark implementation does not overwrite existing data but it will throw error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_product_names_df = spark.createDataFrame(word_count_product_names_rdd)\n",
    "word_count_product_names_df.write.save(product_names_word_count_nb_orc_filename, \\\n",
    "                                       format=\"orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_count_product_names_df = spark.read.orc(product_names_word_count_nb_orc_filename)\n",
    "new_word_count_product_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='daily', _2=1),\n",
       " Row(_1='life', _2=1),\n",
       " Row(_1='scholar', _2=1),\n",
       " Row(_1='shinjiro', _2=1),\n",
       " Row(_1='katsuragi', _2=1),\n",
       " Row(_1='01', _2=1),\n",
       " Row(_1='meruyabookstore', _2=1),\n",
       " Row(_1='stock', _2=2),\n",
       " Row(_1='tas', _2=1),\n",
       " Row(_1='sepatu', _2=3),\n",
       " Row(_1='kacamata', _2=1),\n",
       " Row(_1='kalung', _2=1),\n",
       " Row(_1='baterai', _2=1),\n",
       " Row(_1='pavilion', _2=1),\n",
       " Row(_1='dv3', _2=1),\n",
       " Row(_1='2000', _2=1),\n",
       " Row(_1='cq35', _2=1),\n",
       " Row(_1='black', _2=1),\n",
       " Row(_1='rodford', _2=1),\n",
       " Row(_1='ii', _2=1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_count_product_names_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Word Count using Spark Submit (SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bukalapak-core-ai.big-data-3v.volume-spark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bukalapak-core-ai.big-data-3v.volume-spark.py\n",
    "# Copyright (c) 2018 PT Bukalapak.com\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark\"\n",
    "\n",
    "\n",
    "def tokenize(words):\n",
    "    from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "    return text_to_word_sequence(words['value'])\n",
    "\n",
    "\n",
    "def main(spark):\n",
    "    # Input\n",
    "    product_names_text_filename = \\\n",
    "        \"file:/home/jovyan/work/\" + \\\n",
    "        \"data/product_names_sample/\" + \\\n",
    "        \"product_names.rdd\"\n",
    "    # Output\n",
    "    product_names_word_count_ss_orc_filename = \\\n",
    "        \"file:/home/jovyan/work/\" + \\\n",
    "        \"data/product_names_sample/\" + \\\n",
    "        \"product_names_word_count_ss.orc\"\n",
    "    # Read input\n",
    "    product_names_df = spark.read.text(product_names_text_filename)\n",
    "    product_names_rdd = product_names_df.rdd\n",
    "    # Perform tokenization and word count\n",
    "    tokenized_product_names_rdd = \\\n",
    "        product_names_rdd.flatMap(lambda product_name: tokenize(product_name))\n",
    "    word_count_product_names_rdd = \\\n",
    "        tokenized_product_names_rdd.map(lambda word: (word, 1)) \\\n",
    "                                   .reduceByKey(lambda a, b: a + b)\n",
    "    # Write output\n",
    "    word_count_product_names_df = spark.createDataFrame(word_count_product_names_rdd)\n",
    "    word_count_product_names_df.write.save(product_names_word_count_ss_orc_filename, \\\n",
    "                                           format=\"orc\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(APP_NAME) \\\n",
    "        .getOrCreate()\n",
    "    main(spark)\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Don't forget to delete existing `product_names_word_count_ss.orc` directory in `data/product_names_sample`. Following Spark implementation does not overwrite existing data but it will throw error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-29 15:06:03 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-09-29 15:06:04 INFO  SparkContext:54 - Running Spark version 2.3.1\n",
      "2018-09-29 15:06:04 INFO  SparkContext:54 - Submitted application: bukalapak-core-ai.big-data-3v.volume-spark\n",
      "2018-09-29 15:06:04 INFO  SecurityManager:54 - Changing view acls to: jovyan\n",
      "2018-09-29 15:06:04 INFO  SecurityManager:54 - Changing modify acls to: jovyan\n",
      "2018-09-29 15:06:04 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2018-09-29 15:06:04 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2018-09-29 15:06:04 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "2018-09-29 15:06:04 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 46391.\n",
      "2018-09-29 15:06:04 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2018-09-29 15:06:04 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2018-09-29 15:06:04 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2018-09-29 15:06:04 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2018-09-29 15:06:04 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-a0d664ed-b1cd-4deb-a178-b4c2670e8406\n",
      "2018-09-29 15:06:04 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2018-09-29 15:06:04 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2018-09-29 15:06:04 INFO  log:192 - Logging initialized @1730ms\n",
      "2018-09-29 15:06:04 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2018-09-29 15:06:04 INFO  Server:414 - Started @1855ms\n",
      "2018-09-29 15:06:04 INFO  AbstractConnector:278 - Started ServerConnector@2766a10f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-09-29 15:06:04 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6f7cbc24{/jobs,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2008a71f{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3256bf42{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@12a0cc52{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@77f5a146{/stages,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@70deac55{/stages/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6766570a{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2c2c082a{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@79a7c4a2{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@ccdc1fb{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@87692c6{/storage,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@44d7a6ba{/storage/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@392e856{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@613233c6{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5d6e7d84{/environment,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24f5f2e5{/environment/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30b16e75{/executors,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@40edc7a6{/executors/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6ec10062{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@65a8b585{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5ed1bd83{/static,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@225157ef{/,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6fb1a3a{/api,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@692a123a{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@bc77a74{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:04 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://c7831edbaee2:4040\n",
      "2018-09-29 15:06:04 INFO  SparkContext:54 - Added file file:/home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py at file:/home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py with timestamp 1538233564936\n",
      "2018-09-29 15:06:04 INFO  Utils:54 - Copying /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py to /tmp/spark-6f3c8da3-eb04-4f4f-9bc8-9aa7d5defe2d/userFiles-bc5d9f2b-1671-4455-ab05-76524873dc8b/bukalapak-core-ai.big-data-3v.volume-spark.py\n",
      "2018-09-29 15:06:05 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2018-09-29 15:06:05 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36253.\n",
      "2018-09-29 15:06:05 INFO  NettyBlockTransferService:54 - Server created on c7831edbaee2:36253\n",
      "2018-09-29 15:06:05 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2018-09-29 15:06:05 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, c7831edbaee2, 36253, None)\n",
      "2018-09-29 15:06:05 INFO  BlockManagerMasterEndpoint:54 - Registering block manager c7831edbaee2:36253 with 366.3 MB RAM, BlockManagerId(driver, c7831edbaee2, 36253, None)\n",
      "2018-09-29 15:06:05 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, c7831edbaee2, 36253, None)\n",
      "2018-09-29 15:06:05 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, c7831edbaee2, 36253, None)\n",
      "2018-09-29 15:06:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@502d2de4{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:05 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/work/spark-warehouse/').\n",
      "2018-09-29 15:06:05 INFO  SharedState:54 - Warehouse path is 'file:/home/jovyan/work/spark-warehouse/'.\n",
      "2018-09-29 15:06:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@51a6b5f9{/SQL,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@79777968{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71f31e11{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@653e9c85{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@503198d{/static/sql,null,AVAILABLE,@Spark}\n",
      "2018-09-29 15:06:05 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "2018-09-29 15:06:09 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2018-09-29 15:06:09 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2018-09-29 15:06:09 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\n",
      "2018-09-29 15:06:09 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2018-09-29 15:06:10 INFO  CodeGenerator:54 - Code generated in 762.00617 ms\n",
      "2018-09-29 15:06:10 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 277.7 KB, free 366.0 MB)\n",
      "2018-09-29 15:06:11 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.4 KB, free 366.0 MB)\n",
      "2018-09-29 15:06:11 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on c7831edbaee2:36253 (size: 23.4 KB, free: 366.3 MB)\n",
      "2018-09-29 15:06:11 INFO  SparkContext:54 - Created broadcast 0 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2018-09-29 15:06:11 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2018-09-29 15:06:12 INFO  SparkContext:54 - Starting job: runJob at PythonRDD.scala:149\n",
      "2018-09-29 15:06:12 INFO  DAGScheduler:54 - Registering RDD 5 (reduceByKey at /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py:51)\n",
      "2018-09-29 15:06:12 INFO  DAGScheduler:54 - Got job 0 (runJob at PythonRDD.scala:149) with 1 output partitions\n",
      "2018-09-29 15:06:12 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (runJob at PythonRDD.scala:149)\n",
      "2018-09-29 15:06:12 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 0)\n",
      "2018-09-29 15:06:12 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 0)\n",
      "2018-09-29 15:06:12 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 0 (PairwiseRDD[5] at reduceByKey at /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py:51), which has no missing parents\n",
      "2018-09-29 15:06:12 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 14.0 KB, free 366.0 MB)\n",
      "2018-09-29 15:06:12 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KB, free 366.0 MB)\n",
      "2018-09-29 15:06:12 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on c7831edbaee2:36253 (size: 8.5 KB, free: 366.3 MB)\n",
      "2018-09-29 15:06:12 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\n",
      "2018-09-29 15:06:12 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[5] at reduceByKey at /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py:51) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2018-09-29 15:06:12 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 2 tasks\n",
      "2018-09-29 15:06:12 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8379 bytes)\n",
      "2018-09-29 15:06:12 INFO  TaskSetManager:54 - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 8379 bytes)\n",
      "2018-09-29 15:06:12 INFO  Executor:54 - Running task 1.0 in stage 0.0 (TID 1)\n",
      "2018-09-29 15:06:12 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)\n",
      "2018-09-29 15:06:12 INFO  Executor:54 - Fetching file:/home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py with timestamp 1538233564936\n",
      "2018-09-29 15:06:12 INFO  Utils:54 - /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py has been previously copied to /tmp/spark-6f3c8da3-eb04-4f4f-9bc8-9aa7d5defe2d/userFiles-bc5d9f2b-1671-4455-ab05-76524873dc8b/bukalapak-core-ai.big-data-3v.volume-spark.py\n",
      "2018-09-29 15:06:14 INFO  FileScanRDD:54 - Reading File path: file:///home/jovyan/work/data/product_names_sample/product_names.rdd/part-00000-761349cc-6ec5-4482-bb8d-f328d24686b3-c001.txt, range: 0-244, partition values: [empty row]\n",
      "2018-09-29 15:06:14 INFO  FileScanRDD:54 - Reading File path: file:///home/jovyan/work/data/product_names_sample/product_names.rdd/part-00000-761349cc-6ec5-4482-bb8d-f328d24686b3-c000.txt, range: 0-260, partition values: [empty row]\n",
      "2018-09-29 15:06:14 INFO  CodeGenerator:54 - Code generated in 47.855108 ms\n",
      "2018-09-29 15:06:19 INFO  PythonRunner:54 - Times: total = 5895, boot = 1264, init = 234, finish = 4397\n",
      "2018-09-29 15:06:19 INFO  PythonRunner:54 - Times: total = 5887, boot = 1253, init = 245, finish = 4389\n",
      "2018-09-29 15:06:19 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1908 bytes result sent to driver\n",
      "2018-09-29 15:06:19 INFO  Executor:54 - Finished task 1.0 in stage 0.0 (TID 1). 1908 bytes result sent to driver\n",
      "2018-09-29 15:06:19 INFO  TaskSetManager:54 - Finished task 1.0 in stage 0.0 (TID 1) in 6746 ms on localhost (executor driver) (1/2)\n",
      "2018-09-29 15:06:19 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 6809 ms on localhost (executor driver) (2/2)\n",
      "2018-09-29 15:06:19 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2018-09-29 15:06:19 INFO  DAGScheduler:54 - ShuffleMapStage 0 (reduceByKey at /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark.py:51) finished in 7.081 s\n",
      "2018-09-29 15:06:19 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "2018-09-29 15:06:19 INFO  DAGScheduler:54 - running: Set()\n",
      "2018-09-29 15:06:19 INFO  DAGScheduler:54 - waiting: Set(ResultStage 1)\n",
      "2018-09-29 15:06:19 INFO  DAGScheduler:54 - failed: Set()\n",
      "2018-09-29 15:06:19 INFO  DAGScheduler:54 - Submitting ResultStage 1 (PythonRDD[8] at RDD at PythonRDD.scala:49), which has no missing parents\n",
      "2018-09-29 15:06:19 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 7.4 KB, free 366.0 MB)\n",
      "2018-09-29 15:06:19 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.9 KB, free 366.0 MB)\n",
      "2018-09-29 15:06:19 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on c7831edbaee2:36253 (size: 4.9 KB, free: 366.3 MB)\n",
      "2018-09-29 15:06:19 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1039\n",
      "2018-09-29 15:06:19 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[8] at RDD at PythonRDD.scala:49) (first 15 tasks are for partitions Vector(0))\n",
      "2018-09-29 15:06:19 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 1 tasks\n",
      "2018-09-29 15:06:19 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)\n",
      "2018-09-29 15:06:19 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 2)\n",
      "2018-09-29 15:06:19 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks\n",
      "2018-09-29 15:06:19 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 14 ms\n",
      "2018-09-29 15:06:19 INFO  PythonRunner:54 - Times: total = 23, boot = -616, init = 638, finish = 1\n",
      "2018-09-29 15:06:19 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 2). 1623 bytes result sent to driver\n",
      "2018-09-29 15:06:19 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 2) in 116 ms on localhost (executor driver) (1/1)\n",
      "2018-09-29 15:06:19 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2018-09-29 15:06:19 INFO  DAGScheduler:54 - ResultStage 1 (runJob at PythonRDD.scala:149) finished in 0.150 s\n",
      "2018-09-29 15:06:19 INFO  DAGScheduler:54 - Job 0 finished: runJob at PythonRDD.scala:149, took 7.397215 s\n",
      "2018-09-29 15:06:20 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2018-09-29 15:06:20 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2018-09-29 15:06:20 INFO  SparkContext:54 - Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "2018-09-29 15:06:20 INFO  DAGScheduler:54 - Got job 1 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2018-09-29 15:06:20 INFO  DAGScheduler:54 - Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)\n",
      "2018-09-29 15:06:20 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 2)\n",
      "2018-09-29 15:06:20 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2018-09-29 15:06:20 INFO  DAGScheduler:54 - Submitting ResultStage 3 (MapPartitionsRDD[13] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2018-09-29 15:06:20 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 152.7 KB, free 365.8 MB)\n",
      "2018-09-29 15:06:20 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 56.8 KB, free 365.8 MB)\n",
      "2018-09-29 15:06:20 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on c7831edbaee2:36253 (size: 56.8 KB, free: 366.2 MB)\n",
      "2018-09-29 15:06:20 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1039\n",
      "2018-09-29 15:06:20 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2018-09-29 15:06:20 INFO  TaskSchedulerImpl:54 - Adding task set 3.0 with 2 tasks\n",
      "2018-09-29 15:06:20 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 7649 bytes)\n",
      "2018-09-29 15:06:20 INFO  TaskSetManager:54 - Starting task 1.0 in stage 3.0 (TID 4, localhost, executor driver, partition 1, ANY, 7649 bytes)\n",
      "2018-09-29 15:06:20 INFO  Executor:54 - Running task 0.0 in stage 3.0 (TID 3)\n",
      "2018-09-29 15:06:20 INFO  Executor:54 - Running task 1.0 in stage 3.0 (TID 4)\n",
      "2018-09-29 15:06:20 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks\n",
      "2018-09-29 15:06:20 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 2 ms\n",
      "2018-09-29 15:06:20 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks\n",
      "2018-09-29 15:06:20 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 2 ms\n",
      "2018-09-29 15:06:20 INFO  CodeGenerator:54 - Code generated in 49.157308 ms\n",
      "2018-09-29 15:06:20 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2018-09-29 15:06:20 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2018-09-29 15:06:20 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2018-09-29 15:06:20 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 39\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 25\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 52\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 29\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 23\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 16\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 10\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 26\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 34\n",
      "2018-09-29 15:06:20 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on c7831edbaee2:36253 in memory (size: 4.9 KB, free: 366.2 MB)\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 6\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 19\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 41\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 17\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 15\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 46\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 7\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 27\n",
      "2018-09-29 15:06:20 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on c7831edbaee2:36253 in memory (size: 8.5 KB, free: 366.2 MB)\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 30\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 32\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 33\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 44\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 20\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 28\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 12\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 38\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 55\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 47\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 22\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 43\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 31\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 54\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 37\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 9\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 53\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 21\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 40\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 42\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 14\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 18\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 48\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 36\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 35\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 49\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 45\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 24\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 50\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 11\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 51\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 8\n",
      "2018-09-29 15:06:20 INFO  ContextCleaner:54 - Cleaned accumulator 13\n",
      "2018-09-29 15:06:21 INFO  PythonRunner:54 - Times: total = 8, boot = -1347, init = 1353, finish = 2\n",
      "2018-09-29 15:06:21 INFO  PythonRunner:54 - Times: total = 48, boot = -650, init = 697, finish = 1\n",
      "2018-09-29 15:06:21 INFO  FileOutputCommitter:535 - Saved output of task 'attempt_20180929150620_0003_m_000001_0' to file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc/_temporary/0/task_20180929150620_0003_m_000001\n",
      "2018-09-29 15:06:21 INFO  FileOutputCommitter:535 - Saved output of task 'attempt_20180929150620_0003_m_000000_0' to file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc/_temporary/0/task_20180929150620_0003_m_000000\n",
      "2018-09-29 15:06:21 INFO  SparkHadoopMapRedUtil:54 - attempt_20180929150620_0003_m_000001_0: Committed\n",
      "2018-09-29 15:06:21 INFO  SparkHadoopMapRedUtil:54 - attempt_20180929150620_0003_m_000000_0: Committed\n",
      "2018-09-29 15:06:21 INFO  Executor:54 - Finished task 1.0 in stage 3.0 (TID 4). 2910 bytes result sent to driver\n",
      "2018-09-29 15:06:21 INFO  Executor:54 - Finished task 0.0 in stage 3.0 (TID 3). 2910 bytes result sent to driver\n",
      "2018-09-29 15:06:21 INFO  TaskSetManager:54 - Finished task 1.0 in stage 3.0 (TID 4) in 942 ms on localhost (executor driver) (1/2)\n",
      "2018-09-29 15:06:21 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 945 ms on localhost (executor driver) (2/2)\n",
      "2018-09-29 15:06:21 INFO  TaskSchedulerImpl:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2018-09-29 15:06:21 INFO  DAGScheduler:54 - ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 1.025 s\n",
      "2018-09-29 15:06:21 INFO  DAGScheduler:54 - Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 1.036786 s\n",
      "2018-09-29 15:06:21 INFO  FileFormatWriter:54 - Job null committed.\n",
      "2018-09-29 15:06:21 INFO  FileFormatWriter:54 - Finished processing stats for job null.\n",
      "2018-09-29 15:06:21 INFO  AbstractConnector:318 - Stopped Spark@2766a10f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-09-29 15:06:21 INFO  SparkUI:54 - Stopped Spark web UI at http://c7831edbaee2:4040\n",
      "2018-09-29 15:06:21 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2018-09-29 15:06:21 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2018-09-29 15:06:21 INFO  BlockManager:54 - BlockManager stopped\n",
      "2018-09-29 15:06:21 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2018-09-29 15:06:21 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2018-09-29 15:06:21 INFO  SparkContext:54 - Successfully stopped SparkContext\n",
      "2018-09-29 15:06:22 INFO  ShutdownHookManager:54 - Shutdown hook called\n",
      "2018-09-29 15:06:22 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-6f3c8da3-eb04-4f4f-9bc8-9aa7d5defe2d/pyspark-66290c6c-01ab-4276-8989-07dd9982db7c\n",
      "2018-09-29 15:06:22 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-6f3c8da3-eb04-4f4f-9bc8-9aa7d5defe2d\n",
      "2018-09-29 15:06:22 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-f0727a0a-8e3d-43f2-ac48-e8f6f5929108\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/spark/bin/spark-submit \\\n",
    "    --executor-memory 1g --executor-cores 1 --num-executors 2 \\\n",
    "    bukalapak-core-ai.big-data-3v.volume-spark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/home/jovyan/work/data/product_names_sample/product_names_word_count_ss.orc'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_names_word_count_ss_orc_filename = \\\n",
    "    \"file:/home/jovyan/work/\" + \\\n",
    "    \"data/product_names_sample/\" + \\\n",
    "    \"product_names_word_count_ss.orc\"\n",
    "product_names_word_count_ss_orc_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_count_product_names_df = spark.read.orc(product_names_word_count_ss_orc_filename)\n",
    "new_word_count_product_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='daily', _2=1),\n",
       " Row(_1='life', _2=1),\n",
       " Row(_1='scholar', _2=1),\n",
       " Row(_1='shinjiro', _2=1),\n",
       " Row(_1='katsuragi', _2=1),\n",
       " Row(_1='01', _2=1),\n",
       " Row(_1='meruyabookstore', _2=1),\n",
       " Row(_1='stock', _2=2),\n",
       " Row(_1='tas', _2=1),\n",
       " Row(_1='sepatu', _2=3),\n",
       " Row(_1='kacamata', _2=1),\n",
       " Row(_1='kalung', _2=1),\n",
       " Row(_1='baterai', _2=1),\n",
       " Row(_1='pavilion', _2=1),\n",
       " Row(_1='dv3', _2=1),\n",
       " Row(_1='2000', _2=1),\n",
       " Row(_1='cq35', _2=1),\n",
       " Row(_1='black', _2=1),\n",
       " Row(_1='rodford', _2=1),\n",
       " Row(_1='ii', _2=1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_count_product_names_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
