{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Volume with Apache Spark for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Apache Spark to perform image classification from preprocessed images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 PT Bukalapak.com\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python %s\" % sys.version)\n",
    "import base64\n",
    "import pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1.15.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"NumPy %s\" % np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow %s\" % tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(\"PySpark %s\" % pyspark.__version__)\n",
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Image Classification using Notebook (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built With CUDA: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Built With CUDA:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras FP Precision: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Keras FP Precision:\", tf.keras.backend.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_PROJECT_URL = '/home/jovyan/work/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set config to run Spark locally with 2 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark-img-clsf\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.master\", \"local[2]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5352fae7145a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>bukalapak-core-ai.big-data-3v.volume-spark-img-clsf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=bukalapak-core-ai.big-data-3v.volume-spark-img-clsf>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'bukalapak-core-ai.big-data-3v.volume-spark-img-clsf'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.host', '5352fae7145a'),\n",
       " ('spark.driver.port', '33783'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1565250646357'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.master', 'local[2]')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Preprocessed Images for Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate preprocessed images and store them in HDFS Orc file. Preprocessed images normally come from previous process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ped: preprocessed image\n",
    "def convert_image_url_to_ped(x):\n",
    "    import io, base64, pickle\n",
    "    from PIL import Image as pil_image\n",
    "    import tensorflow as tf\n",
    "    # Convert URL to PIL image\n",
    "    image_url = x[0]\n",
    "    image_pil = pil_image.open(image_url)\n",
    "    # Make Sure the Image is in RGB (not BW)\n",
    "    if image_pil.mode != 'RGB':\n",
    "        image_pil = image_pil.convert('RGB')\n",
    "    # Resize Image\n",
    "    target_size = (224, 224)\n",
    "    width_height_tuple = (target_size[1], target_size[0])\n",
    "    if image_pil.size != width_height_tuple:\n",
    "        image_pil = image_pil.resize(width_height_tuple, pil_image.NEAREST)\n",
    "    # Normalise Image\n",
    "    image_np = tf.keras.preprocessing.image.img_to_array(image_pil)\n",
    "    image_np = tf.keras.applications.vgg16.preprocess_input(image_np)\n",
    "    # Convert numpy array to string\n",
    "    image_ped = base64.b64encode(pickle.dumps(image_np)).decode('UTF-8')\n",
    "    \n",
    "    return [\"local_disk\", image_url, \"jeket\", image_ped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_url_to_ped_orc(image_url_pathfilename, image_ped_orc_pathfilename):\n",
    "    image_url_file_rdd = sc.textFile(image_url_pathfilename)\n",
    "    print(\"        Number of Partitions:\", image_url_file_rdd.getNumPartitions())\n",
    "    image_url_list_rdd = image_url_file_rdd.map(lambda x: x.split('\\n'))\n",
    "    image_ped_list_rdd = image_url_list_rdd.map(lambda x: convert_image_url_to_ped(x))\n",
    "    image_ped_dict_rdd = image_ped_list_rdd.map(lambda x: Row(tid=x[0], \n",
    "                                                              iid=x[1], \n",
    "                                                              l=x[2], \n",
    "                                                              i_ped=x[3]))\n",
    "    image_ped_dict_df = spark.createDataFrame(image_ped_dict_rdd)\n",
    "    image_ped_dict_df.write.save(image_ped_orc_pathfilename, format=\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Number of Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Input file path\n",
    "image_url_pathfilename = \"file:{0}data/image_paths_10.txt\".format(LOCAL_PROJECT_URL)\n",
    "# Output file path\n",
    "image_ped_orc_directory = \"{0}data/images_ped.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_ped_orc_pathfilename = \"file:%s\" % image_ped_orc_directory\n",
    "# Remove existing output directory\n",
    "shutil.rmtree(image_ped_orc_directory, ignore_errors=True)\n",
    "# Start preprocessing\n",
    "convert_image_url_to_ped_orc(image_url_pathfilename, image_ped_orc_pathfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making inference using VGG16 model[1] pretrained with imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is the list of [tid, iid, l, i_ped] from preprocessed image table\n",
    "# output is the list of [tid, iid, pred] for inference result table\n",
    "# where:\n",
    "#  - tid: table ID\n",
    "#  - iid: image ID\n",
    "#  - l: label\n",
    "#  - i_ped: preprocessed image\n",
    "#  - pred: output of prediction layer (the inference result)\n",
    "\n",
    "def make_inference(xs):\n",
    "    import base64, pickle\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    # Extract input lists\n",
    "    inference_lists = []\n",
    "    images_array = []\n",
    "    for x in xs:\n",
    "        inference_lists.append([x.tid, x.iid])\n",
    "        images_array.append(pickle.loads(base64.b64decode(x.i_ped.encode('UTF-8'))))\n",
    "    images_np = np.array(images_array)\n",
    "    # Load VGG16 model\n",
    "    vgg = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "    # Do inference\n",
    "    inference = vgg.predict(images_np)\n",
    "    # Add inference results to inference table lists\n",
    "    if len(inference_lists) != len(inference):\n",
    "        raise ValueError('The total of inference table lists is not ' +\n",
    "                         'the same as the total of inference result')\n",
    "    for i in range(len(inference_lists)):\n",
    "        inference_lists[i].append(\n",
    "            base64.b64encode(pickle.dumps(inference[i])).decode('UTF-8')\n",
    "        )\n",
    "\n",
    "    return iter(inference_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_ped_orc_to_infr_orc(image_ped_orc_pathfilename,\n",
    "                                      image_infr_orc_pathfilename):\n",
    "    image_ped_dict_df = spark.read.orc(image_ped_orc_pathfilename)\n",
    "    image_ped_dict_rdd = image_ped_dict_df.rdd\n",
    "    print(\"        Number of Partitions:\", image_ped_dict_rdd.getNumPartitions())\n",
    "    image_infr_list_rdd = image_ped_dict_rdd.mapPartitions(make_inference)\n",
    "    image_infr_dict_rdd = image_infr_list_rdd.map(lambda x: Row(tid=x[0],\n",
    "                                                                iid=x[1],\n",
    "                                                                pred=x[2]))\n",
    "    image_infr_dict_df = spark.createDataFrame(image_infr_dict_rdd)\n",
    "    image_infr_dict_df.write.save(image_infr_orc_pathfilename, format=\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Number of Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Input file path\n",
    "image_ped_orc_directory = \"{0}data/images_ped.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_ped_orc_pathfilename = \"file:{0}\".format(image_ped_orc_directory)\n",
    "# Output file path\n",
    "image_infr_orc_directory = \"{0}data/images_infr.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_infr_orc_pathfilename = \"file:{0}\".format(image_infr_orc_directory)\n",
    "# Remove existing output directory\n",
    "shutil.rmtree(image_infr_orc_directory, ignore_errors=True)\n",
    "# Start making inference\n",
    "convert_image_ped_orc_to_infr_orc(image_ped_orc_pathfilename, \n",
    "                                  image_infr_orc_pathfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read inference result\n",
    "image_infr_orc_directory = \"{0}data/images_infr.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_infr_orc_pathfilename = \"file:{0}\".format(image_infr_orc_directory)\n",
    "image_infr_dict_df = spark.read.orc(image_infr_orc_pathfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_infr_dict_rdd = image_infr_dict_df.rdd\n",
    "inference_lists = image_infr_dict_rdd.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local_disk'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists.tid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/images/jacket copy 0.png'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists.iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gANjbnVtcHkuY29yZS5tdWx0aWFycmF5Cl9yZWNvbnN0cnVjdApxAGNudW1weQpuZGFycmF5CnEBSwCFcQJDAWJxA4dxBFJxBShLAU3oA4VxBmNudW1weQpkdHlwZQpxB1gCAAAAZjRxCEsASwGHcQlScQooSwNYAQAAADxxC05OTkr/////Sv////9LAHRxDGKJQqAPAACD9II0cjczMPWpKzJ0zekxmvgHMzXB/TIkyTUxdL+pMP67JjDn7zUwqVEuMHolbC8U8SIvAOIeMOmOOTGMbyAvF56MMJb8tTFN'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists.pred[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pbs_to_infr_tensor(pbs):\n",
    "    return np.array(\n",
    "        [list(pickle.loads(base64.b64decode(pbs.encode('UTF-8'))))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n04370456', 'sweatshirt', 0.971396),\n",
       "  ('n04599235', 'wool', 0.015504221),\n",
       "  ('n03595614', 'jersey', 0.005360415),\n",
       "  ('n02963159', 'cardigan', 0.0013214782),\n",
       "  ('n03594734', 'jean', 0.0011332377)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.applications.vgg16.decode_predictions(\n",
    "    convert_pbs_to_infr_tensor(inference_lists.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:\n",
    "```\n",
    "[[('n04370456', 'sweatshirt', 0.971396),\n",
    "  ('n04599235', 'wool', 0.015504221),\n",
    "  ('n03595614', 'jersey', 0.005360415),\n",
    "  ('n02963159', 'cardigan', 0.0013214782),\n",
    "  ('n03594734', 'jean', 0.0011332377)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Image Classification using Spark Submit (SS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use previously generated [preprocessed images](#Generate-Preprocessed-Images-for-Input) for the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py\n",
    "# Copyright (c) 2019 PT Bukalapak.com\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark-img-clsf\"\n",
    "\n",
    "\n",
    "def make_inference(xs):\n",
    "    import base64, pickle\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    # Extract input lists\n",
    "    inference_lists = []\n",
    "    images_array = []\n",
    "    for x in xs:\n",
    "        inference_lists.append([x.tid, x.iid])\n",
    "        images_array.append(pickle.loads(base64.b64decode(x.i_ped.encode('UTF-8'))))\n",
    "    images_np = np.array(images_array)\n",
    "    # Load VGG16 model\n",
    "    vgg = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "    # Do inference\n",
    "    inference = vgg.predict(images_np)\n",
    "    # Add inference results to inference table lists\n",
    "    if len(inference_lists) != len(inference):\n",
    "        raise ValueError('The total of inference table lists is not ' +\n",
    "                         'the same as the total of inference result')\n",
    "    for i in range(len(inference_lists)):\n",
    "        inference_lists[i].append(\n",
    "            base64.b64encode(pickle.dumps(inference[i])).decode('UTF-8')\n",
    "        )\n",
    "\n",
    "    return iter(inference_lists)\n",
    "\n",
    "\n",
    "def main(spark):\n",
    "    from pyspark.sql import Row\n",
    "    # Input\n",
    "    image_ped_orc_pathfilename = \\\n",
    "        \"file:/home/jovyan/work/\" + \\\n",
    "        \"data/images_ped.orc\"\n",
    "    # Output\n",
    "    image_infr_orc_pathfilename = \\\n",
    "        \"file:/home/jovyan/work/\" + \\\n",
    "        \"data/images_infr_ss.orc\"\n",
    "    # Read input file\n",
    "    image_ped_dict_df = spark.read.orc(image_ped_orc_pathfilename)\n",
    "    image_ped_dict_rdd = image_ped_dict_df.rdd\n",
    "    print(\"        Number of Partitions:\", image_ped_dict_rdd.getNumPartitions())\n",
    "    # Perform inference\n",
    "    image_infr_list_rdd = image_ped_dict_rdd.mapPartitions(make_inference)\n",
    "    # Write output file\n",
    "    image_infr_dict_rdd = image_infr_list_rdd.map(lambda x: Row(tid=x[0],\n",
    "                                                                iid=x[1],\n",
    "                                                                pred=x[2]))\n",
    "    image_infr_dict_df = spark.createDataFrame(image_infr_dict_rdd)\n",
    "    image_infr_dict_df.write.save(image_infr_orc_pathfilename, format=\"orc\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(APP_NAME) \\\n",
    "        .getOrCreate()\n",
    "    main(spark)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Don't forget to delete existing `images_infr_ss.orc` directory in `data/`. Following Spark implementation does not overwrite existing data but it will throw error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Initial run will take very long as the Spark code needs to download about 500 MB of VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 08:02:30 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2019-08-08 08:02:31 INFO  SparkContext:54 - Running Spark version 2.3.3\n",
      "2019-08-08 08:02:31 INFO  SparkContext:54 - Submitted application: bukalapak-core-ai.big-data-3v.volume-spark-img-clsf\n",
      "2019-08-08 08:02:31 INFO  SecurityManager:54 - Changing view acls to: jovyan\n",
      "2019-08-08 08:02:31 INFO  SecurityManager:54 - Changing modify acls to: jovyan\n",
      "2019-08-08 08:02:31 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2019-08-08 08:02:31 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2019-08-08 08:02:31 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "2019-08-08 08:02:31 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 39007.\n",
      "2019-08-08 08:02:31 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2019-08-08 08:02:31 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2019-08-08 08:02:31 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2019-08-08 08:02:31 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2019-08-08 08:02:31 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-86a72e99-3bcf-4a2e-b9af-3a9e0c3429a0\n",
      "2019-08-08 08:02:31 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2019-08-08 08:02:31 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2019-08-08 08:02:31 INFO  log:192 - Logging initialized @1449ms\n",
      "2019-08-08 08:02:31 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "2019-08-08 08:02:31 INFO  Server:419 - Started @1498ms\n",
      "2019-08-08 08:02:31 INFO  AbstractConnector:278 - Started ServerConnector@2a5fe865{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2019-08-08 08:02:31 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@e13c049{/jobs,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@12467089{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2030d7a7{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@38ec3a73{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@190fc7df{/stages,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3fa4a6b6{/stages/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6feff8{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24d4f78e{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bbbccc3{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@49310dc2{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@c14bb5e{/storage,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@fff45d5{/storage/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@aa74d3d{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b391eb6{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62de92c4{/environment,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@17faa2a7{/environment/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2784dbc0{/executors,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24cbbd28{/executors/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@321553c7{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@70b820b8{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71e3afb4{/static,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1bead211{/,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@129ea728{/api,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@78915363{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3ba897b8{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://5352fae7145a:4040\n",
      "2019-08-08 08:02:31 INFO  SparkContext:54 - Added file file:/home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py at file:/home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py with timestamp 1565251351600\n",
      "2019-08-08 08:02:31 INFO  Utils:54 - Copying /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py to /tmp/spark-d590e0c8-4657-4ae3-b3df-019d355ea9bc/userFiles-fef58b24-f6af-465b-a1b3-c6e75dd85d22/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py\n",
      "2019-08-08 08:02:31 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2019-08-08 08:02:31 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44651.\n",
      "2019-08-08 08:02:31 INFO  NettyBlockTransferService:54 - Server created on 5352fae7145a:44651\n",
      "2019-08-08 08:02:31 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2019-08-08 08:02:31 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 5352fae7145a, 44651, None)\n",
      "2019-08-08 08:02:31 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 5352fae7145a:44651 with 366.3 MB RAM, BlockManagerId(driver, 5352fae7145a, 44651, None)\n",
      "2019-08-08 08:02:31 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 5352fae7145a, 44651, None)\n",
      "2019-08-08 08:02:31 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 5352fae7145a, 44651, None)\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@16cf5685{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/work/spark-warehouse/').\n",
      "2019-08-08 08:02:31 INFO  SharedState:54 - Warehouse path is 'file:/home/jovyan/work/spark-warehouse/'.\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@c1de5e9{/SQL,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4e53f5d0{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5cd64ada{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@74a3d78b{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6eddad77{/static/sql,null,AVAILABLE,@Spark}\n",
      "2019-08-08 08:02:32 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "2019-08-08 08:02:33 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-08-08 08:02:33 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2019-08-08 08:02:33 INFO  FileSourceStrategy:54 - Output Data Schema: struct<i_ped: string, iid: string, l: string, tid: string ... 2 more fields>\n",
      "2019-08-08 08:02:33 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-08-08 08:02:33 INFO  CodeGenerator:54 - Code generated in 120.020269 ms\n",
      "2019-08-08 08:02:33 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 277.6 KB, free 366.0 MB)\n",
      "2019-08-08 08:02:33 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.4 KB, free 366.0 MB)\n",
      "2019-08-08 08:02:33 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 5352fae7145a:44651 (size: 23.4 KB, free: 366.3 MB)\n",
      "2019-08-08 08:02:33 INFO  SparkContext:54 - Created broadcast 0 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2019-08-08 08:02:33 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "        Number of Partitions: 2\n",
      "2019-08-08 08:02:34 INFO  SparkContext:54 - Starting job: runJob at PythonRDD.scala:152\n",
      "2019-08-08 08:02:34 INFO  DAGScheduler:54 - Got job 0 (runJob at PythonRDD.scala:152) with 1 output partitions\n",
      "2019-08-08 08:02:34 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (runJob at PythonRDD.scala:152)\n",
      "2019-08-08 08:02:34 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 08:02:34 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 08:02:34 INFO  DAGScheduler:54 - Submitting ResultStage 0 (PythonRDD[4] at RDD at PythonRDD.scala:52), which has no missing parents\n",
      "2019-08-08 08:02:34 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 11.9 KB, free 366.0 MB)\n",
      "2019-08-08 08:02:34 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.1 KB, free 366.0 MB)\n",
      "2019-08-08 08:02:34 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 5352fae7145a:44651 (size: 7.1 KB, free: 366.3 MB)\n",
      "2019-08-08 08:02:34 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\n",
      "2019-08-08 08:02:34 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at RDD at PythonRDD.scala:52) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 08:02:34 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks\n",
      "2019-08-08 08:02:34 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8373 bytes)\n",
      "2019-08-08 08:02:34 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)\n",
      "2019-08-08 08:02:34 INFO  Executor:54 - Fetching file:/home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py with timestamp 1565251351600\n",
      "2019-08-08 08:02:34 INFO  Utils:54 - /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py has been previously copied to /tmp/spark-d590e0c8-4657-4ae3-b3df-019d355ea9bc/userFiles-fef58b24-f6af-465b-a1b3-c6e75dd85d22/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py\n",
      "2019-08-08 08:02:34 INFO  FileScanRDD:54 - Reading File path: file:///home/jovyan/work/data/images_ped.orc/part-00000-473c0e36-e6c6-43cb-a798-775a982d5148-c000.snappy.orc, range: 0-1797187, partition values: [empty row]\n",
      "2019-08-08 08:02:34 INFO  CodeGenerator:54 - Code generated in 16.256646 ms\n",
      "2019-08-08 08:02:34 INFO  ReaderImpl:531 - Reading ORC rows from file:/home/jovyan/work/data/images_ped.orc/part-00000-473c0e36-e6c6-43cb-a798-775a982d5148-c000.snappy.orc with {include: [true, true, true, true, true], offset: 0, length: 1797187}\n",
      "2019-08-08 08:02:34 INFO  RecordReaderFactory:90 - Schema is not specified on read. Using file schema.\n",
      "2019-08-08 08:02:38 INFO  PythonRunner:54 - Times: total = 4205, boot = 360, init = 164, finish = 3681\n",
      "2019-08-08 08:02:38 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 7585 bytes result sent to driver\n",
      "2019-08-08 08:02:38 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 4309 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 08:02:38 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2019-08-08 08:02:38 INFO  PythonAccumulatorV2:54 - Connected to AccumulatorServer at host: 127.0.0.1 port: 44701\n",
      "2019-08-08 08:02:38 INFO  DAGScheduler:54 - ResultStage 0 (runJob at PythonRDD.scala:152) finished in 4.368 s\n",
      "2019-08-08 08:02:38 INFO  DAGScheduler:54 - Job 0 finished: runJob at PythonRDD.scala:152, took 4.392608 s\n",
      "2019-08-08 08:02:38 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2019-08-08 08:02:38 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2019-08-08 08:02:38 INFO  SparkContext:54 - Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "2019-08-08 08:02:38 INFO  DAGScheduler:54 - Got job 1 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2019-08-08 08:02:38 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (save at NativeMethodAccessorImpl.java:0)\n",
      "2019-08-08 08:02:38 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 08:02:38 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 08:02:38 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2019-08-08 08:02:38 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 155.5 KB, free 365.8 MB)\n",
      "2019-08-08 08:02:38 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 58.1 KB, free 365.8 MB)\n",
      "2019-08-08 08:02:38 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 5352fae7145a:44651 (size: 58.1 KB, free: 366.2 MB)\n",
      "2019-08-08 08:02:38 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1039\n",
      "2019-08-08 08:02:38 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2019-08-08 08:02:38 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 2 tasks\n",
      "2019-08-08 08:02:38 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8373 bytes)\n",
      "2019-08-08 08:02:38 INFO  TaskSetManager:54 - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 8373 bytes)\n",
      "2019-08-08 08:02:38 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)\n",
      "2019-08-08 08:02:38 INFO  Executor:54 - Running task 1.0 in stage 1.0 (TID 2)\n",
      "2019-08-08 08:02:38 INFO  FileScanRDD:54 - Reading File path: file:///home/jovyan/work/data/images_ped.orc/part-00000-473c0e36-e6c6-43cb-a798-775a982d5148-c000.snappy.orc, range: 0-1797187, partition values: [empty row]\n",
      "2019-08-08 08:02:38 INFO  FileScanRDD:54 - Reading File path: file:///home/jovyan/work/data/images_ped.orc/part-00001-473c0e36-e6c6-43cb-a798-775a982d5148-c000.snappy.orc, range: 0-1797175, partition values: [empty row]\n",
      "2019-08-08 08:02:38 INFO  CodeGenerator:54 - Code generated in 10.714315 ms\n",
      "2019-08-08 08:02:38 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2019-08-08 08:02:38 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2019-08-08 08:02:38 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2019-08-08 08:02:38 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 24\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 26\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 19\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 15\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 13\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 30\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 25\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 10\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 16\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 21\n",
      "2019-08-08 08:02:38 INFO  ReaderImpl:531 - Reading ORC rows from file:/home/jovyan/work/data/images_ped.orc/part-00001-473c0e36-e6c6-43cb-a798-775a982d5148-c000.snappy.orc with {include: [true, true, true, true, true], offset: 0, length: 1797175}\n",
      "2019-08-08 08:02:38 INFO  RecordReaderFactory:90 - Schema is not specified on read. Using file schema.\n",
      "2019-08-08 08:02:38 INFO  ReaderImpl:531 - Reading ORC rows from file:/home/jovyan/work/data/images_ped.orc/part-00000-473c0e36-e6c6-43cb-a798-775a982d5148-c000.snappy.orc with {include: [true, true, true, true, true], offset: 0, length: 1797187}\n",
      "2019-08-08 08:02:38 INFO  RecordReaderFactory:90 - Schema is not specified on read. Using file schema.\n",
      "2019-08-08 08:02:38 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 5352fae7145a:44651 in memory (size: 7.1 KB, free: 366.2 MB)\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 9\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 29\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 22\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 8\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 11\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 14\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 12\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 20\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 23\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 18\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 6\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 27\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 7\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 17\n",
      "2019-08-08 08:02:38 INFO  ContextCleaner:54 - Cleaned accumulator 28\n",
      "2019-08-08 08:02:41 INFO  PythonRunner:54 - Times: total = 2709, boot = -161, init = 218, finish = 2652\n",
      "2019-08-08 08:02:41 INFO  FileOutputCommitter:535 - Saved output of task 'attempt_20190808080238_0001_m_000000_1' to file:/home/jovyan/work/data/images_infr_ss.orc/_temporary/0/task_20190808080238_0001_m_000000\n",
      "2019-08-08 08:02:41 INFO  SparkHadoopMapRedUtil:54 - attempt_20190808080238_0001_m_000000_1: Committed\n",
      "2019-08-08 08:02:41 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 1). 2986 bytes result sent to driver\n",
      "2019-08-08 08:02:41 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 2819 ms on localhost (executor driver) (1/2)\n",
      "2019-08-08 08:02:42 INFO  PythonRunner:54 - Times: total = 4138, boot = 3, init = 52, finish = 4083\n",
      "2019-08-08 08:02:42 INFO  FileOutputCommitter:535 - Saved output of task 'attempt_20190808080238_0001_m_000001_2' to file:/home/jovyan/work/data/images_infr_ss.orc/_temporary/0/task_20190808080238_0001_m_000001\n",
      "2019-08-08 08:02:42 INFO  SparkHadoopMapRedUtil:54 - attempt_20190808080238_0001_m_000001_2: Committed\n",
      "2019-08-08 08:02:42 INFO  Executor:54 - Finished task 1.0 in stage 1.0 (TID 2). 2943 bytes result sent to driver\n",
      "2019-08-08 08:02:42 INFO  TaskSetManager:54 - Finished task 1.0 in stage 1.0 (TID 2) in 4193 ms on localhost (executor driver) (2/2)\n",
      "2019-08-08 08:02:42 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2019-08-08 08:02:42 INFO  DAGScheduler:54 - ResultStage 1 (save at NativeMethodAccessorImpl.java:0) finished in 4.217 s\n",
      "2019-08-08 08:02:42 INFO  DAGScheduler:54 - Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 4.220227 s\n",
      "2019-08-08 08:02:42 INFO  FileFormatWriter:54 - Job null committed.\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 54\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 55\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 39\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 53\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 52\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 44\n",
      "2019-08-08 08:02:42 INFO  FileFormatWriter:54 - Finished processing stats for job null.\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 57\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 49\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 51\n",
      "2019-08-08 08:02:42 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on 5352fae7145a:44651 in memory (size: 58.1 KB, free: 366.3 MB)\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 60\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 40\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 43\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 58\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 56\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 46\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 48\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 50\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 41\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 42\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 36\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 38\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 47\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 45\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 37\n",
      "2019-08-08 08:02:42 INFO  ContextCleaner:54 - Cleaned accumulator 59\n",
      "2019-08-08 08:02:42 INFO  AbstractConnector:318 - Stopped Spark@2a5fe865{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2019-08-08 08:02:42 INFO  SparkUI:54 - Stopped Spark web UI at http://5352fae7145a:4040\n",
      "2019-08-08 08:02:42 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2019-08-08 08:02:42 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2019-08-08 08:02:42 INFO  BlockManager:54 - BlockManager stopped\n",
      "2019-08-08 08:02:42 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2019-08-08 08:02:42 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2019-08-08 08:02:42 INFO  SparkContext:54 - Successfully stopped SparkContext\n",
      "2019-08-08 08:02:43 INFO  ShutdownHookManager:54 - Shutdown hook called\n",
      "2019-08-08 08:02:43 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-cc7b8f11-2b21-4196-b97f-ac92fc500fc4\n",
      "2019-08-08 08:02:43 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-d590e0c8-4657-4ae3-b3df-019d355ea9bc\n",
      "2019-08-08 08:02:43 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-d590e0c8-4657-4ae3-b3df-019d355ea9bc/pyspark-a02396d3-7fca-400b-9684-f5fafbffb957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0808 08:02:35.863696 140077526464320 deprecation.py:506] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "2019-08-08 08:02:37.087386: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-08-08 08:02:37.107633: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\n",
      "2019-08-08 08:02:37.108259: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b123840ef0 executing computations on platform Host. Devices:\n",
      "2019-08-08 08:02:37.108277: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-08-08 08:02:37.138957: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0808 08:02:39.860407 140077526464320 deprecation.py:506] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "2019-08-08 08:02:41.454950: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-08-08 08:02:41.475627: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\n",
      "2019-08-08 08:02:41.476523: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b123840ef0 executing computations on platform Host. Devices:\n",
      "2019-08-08 08:02:41.476578: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-08-08 08:02:41.511913: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "2019-08-08 08:02:41.563424: W tensorflow/core/framework/allocator.cc:107] Allocation of 411041792 exceeds 10% of system memory.\n",
      "2019-08-08 08:02:41.829783: W tensorflow/core/framework/allocator.cc:107] Allocation of 411041792 exceeds 10% of system memory.\n",
      "2019-08-08 08:02:42.020580: W tensorflow/core/framework/allocator.cc:107] Allocation of 411041792 exceeds 10% of system memory.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/spark/bin/spark-submit \\\n",
    "    --executor-memory 2g --executor-cores 1 --num-executors 2 \\\n",
    "    bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Settings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_PROJECT_URL = '/home/jovyan/work/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Spark Context__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set config to run Spark locally with 2 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark-img-clsf\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.master\", \"local[2]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read Inference Result__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_infr_orc_directory = \"{0}data/images_infr_ss.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_infr_orc_pathfilename = \"file:{0}\".format(image_infr_orc_directory)\n",
    "image_infr_dict_df = spark.read.orc(image_infr_orc_pathfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_infr_dict_rdd = image_infr_dict_df.rdd\n",
    "inference_lists = image_infr_dict_rdd.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local_disk'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists.tid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/images/jacket copy 0.png'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists.iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gANjbnVtcHkuY29yZS5tdWx0aWFycmF5Cl9yZWNvbnN0cnVjdApxAGNudW1weQpuZGFycmF5CnEBSwCFcQJDAWJxA4dxBFJxBShLAU3oA4VxBmNudW1weQpkdHlwZQpxB1gCAAAAZjRxCEsASwGHcQlScQooSwNYAQAAADxxC05OTkr/////Sv////9LAHRxDGKJQqAPAACD9II0cjczMPWpKzJ0zekxmvgHMzXB/TIkyTUxdL+pMP67JjDn7zUwqVEuMHolbC8U8SIvAOIeMOmOOTGMbyAvF56MMJb8tTFN'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists.pred[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pbs_to_infr_tensor(pbs):\n",
    "    return np.array(\n",
    "        [list(pickle.loads(base64.b64decode(pbs.encode('UTF-8'))))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n04370456', 'sweatshirt', 0.971396),\n",
       "  ('n04599235', 'wool', 0.015504221),\n",
       "  ('n03595614', 'jersey', 0.005360415),\n",
       "  ('n02963159', 'cardigan', 0.0013214782),\n",
       "  ('n03594734', 'jean', 0.0011332377)]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.applications.vgg16.decode_predictions(\n",
    "    convert_pbs_to_infr_tensor(inference_lists.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:\n",
    "```\n",
    "[[('n04370456', 'sweatshirt', 0.971396),\n",
    "  ('n04599235', 'wool', 0.015504221),\n",
    "  ('n03595614', 'jersey', 0.005360415),\n",
    "  ('n02963159', 'cardigan', 0.0013214782),\n",
    "  ('n03594734', 'jean', 0.0011332377)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Close Spark Context__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. K Simonyan, A Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. CoRR, abs/1409.1556. http://arxiv.org/abs/1409.1556."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
