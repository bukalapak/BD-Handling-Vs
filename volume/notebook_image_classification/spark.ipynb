{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Volume with Apache Spark for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Apache Spark to perform image classification from preprocessed images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 PT Bukalapak.com\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python %s\" % sys.version)\n",
    "import base64\n",
    "import pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1.15.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"NumPy %s\" % np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow %s\" % tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(\"PySpark %s\" % pyspark.__version__)\n",
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Image Classification using Notebook (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built With CUDA: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Built With CUDA:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras FP Precision: float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Keras FP Precision:\", tf.keras.backend.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_PROJECT_URL = '/home/jovyan/work/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set config to run Spark locally with 2 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark-img-clsf\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.master\", \"local[2]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://00009bd93b41:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>bukalapak-core-ai.big-data-3v.volume-spark-img-clsf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=bukalapak-core-ai.big-data-3v.volume-spark-img-clsf>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '40423'),\n",
       " ('spark.app.id', 'local-1564515444154'),\n",
       " ('spark.app.name', 'bukalapak-core-ai.big-data-3v.volume-spark-img-clsf'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '00009bd93b41'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.master', 'local[2]')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Preprocessed Images for Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate preprocessed images and store them in HDFS Orc file. Preprocessed images normally come from previous process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ped: preprocessed image\n",
    "def convert_image_url_to_ped(x):\n",
    "    import io, base64, pickle\n",
    "    from PIL import Image as pil_image\n",
    "    import tensorflow as tf\n",
    "    # Convert URL to PIL image\n",
    "    image_url = x[0]\n",
    "    image_pil = pil_image.open(image_url)\n",
    "    # Make Sure the Image is in RGB (not BW)\n",
    "    if image_pil.mode != 'RGB':\n",
    "        image_pil = image_pil.convert('RGB')\n",
    "    # Resize Image\n",
    "    target_size = (224, 224)\n",
    "    width_height_tuple = (target_size[1], target_size[0])\n",
    "    if image_pil.size != width_height_tuple:\n",
    "        image_pil = image_pil.resize(width_height_tuple, pil_image.NEAREST)\n",
    "    # Normalise Image\n",
    "    image_np = tf.keras.preprocessing.image.img_to_array(image_pil)\n",
    "    image_np = tf.keras.applications.vgg16.preprocess_input(image_np)\n",
    "    # Convert numpy array to string\n",
    "    image_ped = base64.b64encode(pickle.dumps(image_np)).decode('UTF-8')\n",
    "    \n",
    "    return [\"local_disk\", image_url, \"jeket\", image_ped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_url_to_ped_orc(image_url_pathfilename, image_ped_orc_pathfilename):\n",
    "    image_url_file_rdd = sc.textFile(image_url_pathfilename)\n",
    "    print(\"        Number of Partitions:\", image_url_file_rdd.getNumPartitions())\n",
    "    image_url_list_rdd = image_url_file_rdd.map(lambda x: x.split('\\n'))\n",
    "    image_ped_list_rdd = image_url_list_rdd.map(lambda x: convert_image_url_to_ped(x))\n",
    "    image_ped_dict_rdd = image_ped_list_rdd.map(lambda x: Row(tid=x[0], \n",
    "                                                              iid=x[1], \n",
    "                                                              l=x[2], \n",
    "                                                              i_ped=x[3]))\n",
    "    image_ped_dict_df = spark.createDataFrame(image_ped_dict_rdd)\n",
    "    image_ped_dict_df.write.save(image_ped_orc_pathfilename, format=\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Number of Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Input file path\n",
    "image_url_pathfilename = \"file:{0}data/image_paths_10.txt\".format(LOCAL_PROJECT_URL)\n",
    "# Output file path\n",
    "image_ped_orc_directory = \"{0}data/images_ped.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_ped_orc_pathfilename = \"file:%s\" % image_ped_orc_directory\n",
    "# Remove existing output directory\n",
    "shutil.rmtree(image_ped_orc_directory, ignore_errors=True)\n",
    "# Start preprocessing\n",
    "convert_image_url_to_ped_orc(image_url_pathfilename, image_ped_orc_pathfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making inference using VGG16 model[1] pretrained with imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is the list of [tid, iid, l, i_ped] from preprocessed image table\n",
    "# output is the list of [tid, iid, pred] for inference result table\n",
    "# where:\n",
    "#  - tid: table ID\n",
    "#  - iid: image ID\n",
    "#  - l: label\n",
    "#  - i_ped: preprocessed image\n",
    "#  - pred: output of prediction layer (the inference result)\n",
    "\n",
    "def make_inference(xs):\n",
    "    import base64, pickle\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    # Extract input lists\n",
    "    inference_lists = []\n",
    "    images_array = []\n",
    "    for x in xs:\n",
    "        inference_lists.append([x.tid, x.iid])\n",
    "        images_array.append(pickle.loads(base64.b64decode(x.i_ped.encode('UTF-8'))))\n",
    "    images_np = np.array(images_array)\n",
    "    # Load VGG16 model\n",
    "    vgg = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "    # Do inference\n",
    "    inference = vgg.predict(images_np)\n",
    "    # Add inference results to inference table lists\n",
    "    if len(inference_lists) != len(inference):\n",
    "        raise ValueError('The total of inference table lists is not ' +\n",
    "                         'the same as the total of inference result')\n",
    "    for i in range(len(inference_lists)):\n",
    "        inference_lists[i].append(\n",
    "            base64.b64encode(pickle.dumps(inference[i])).decode('UTF-8')\n",
    "        )\n",
    "\n",
    "    return iter(inference_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_ped_orc_to_infr_orc(image_ped_orc_pathfilename,\n",
    "                                      image_infr_orc_pathfilename):\n",
    "    image_ped_dict_df = spark.read.orc(image_ped_orc_pathfilename)\n",
    "    image_ped_dict_rdd = image_ped_dict_df.rdd\n",
    "    print(\"        Number of Partitions:\", image_ped_dict_rdd.getNumPartitions())\n",
    "    image_infr_list_rdd = image_ped_dict_rdd.mapPartitions(make_inference)\n",
    "    image_infr_dict_rdd = image_infr_list_rdd.map(lambda x: Row(tid=x[0],\n",
    "                                                                iid=x[1],\n",
    "                                                                pred=x[2]))\n",
    "    image_infr_dict_df = spark.createDataFrame(image_infr_dict_rdd)\n",
    "    image_infr_dict_df.write.save(image_infr_orc_pathfilename, format=\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Number of Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Input file path\n",
    "image_ped_orc_directory = \"{0}data/images_ped.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_ped_orc_pathfilename = \"file:{0}\".format(image_ped_orc_directory)\n",
    "# Output file path\n",
    "image_infr_orc_directory = \"{0}data/images_infr.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_infr_orc_pathfilename = \"file:{0}\".format(image_infr_orc_directory)\n",
    "# Remove existing output directory\n",
    "shutil.rmtree(image_infr_orc_directory, ignore_errors=True)\n",
    "# Start making inference\n",
    "convert_image_ped_orc_to_infr_orc(image_ped_orc_pathfilename, \n",
    "                                  image_infr_orc_pathfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read inference result\n",
    "image_infr_orc_directory = \"{0}data/images_infr.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_infr_orc_pathfilename = \"file:{0}\".format(image_infr_orc_directory)\n",
    "image_infr_dict_df = spark.read.orc(image_infr_orc_pathfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_infr_dict_rdd = image_infr_dict_df.rdd\n",
    "inference_lists = image_infr_dict_rdd.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/images/jacket copy 0.png'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gANjbnVtcHkuY29yZS5tdWx0aWFycmF5Cl9yZWNvbnN0cnVjdApxAGNudW1weQpuZGFycmF5CnEBSwCFcQJDAWJxA4dxBFJxBShL'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists[1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pbs_to_infr_tensor(pbs):\n",
    "    return np.array(\n",
    "        [list(pickle.loads(base64.b64decode(pbs.encode('UTF-8'))))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n04370456', 'sweatshirt', 0.971396),\n",
       "  ('n04599235', 'wool', 0.015504221),\n",
       "  ('n03595614', 'jersey', 0.005360415),\n",
       "  ('n02963159', 'cardigan', 0.0013214782),\n",
       "  ('n03594734', 'jean', 0.0011332377)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.applications.vgg16.decode_predictions(\n",
    "    convert_pbs_to_infr_tensor(inference_lists[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:\n",
    "```\n",
    "[[('n04370456', 'sweatshirt', 0.971396),\n",
    "  ('n04599235', 'wool', 0.015504221),\n",
    "  ('n03595614', 'jersey', 0.005360415),\n",
    "  ('n02963159', 'cardigan', 0.0013214782),\n",
    "  ('n03594734', 'jean', 0.0011332377)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Image Classification using Spark Submit (SS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use previously generated [preprocessed images](#Generate-Preprocessed-Images-for-Input) for the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py\n",
    "# Copyright (c) 2019 PT Bukalapak.com\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark-img-clsf\"\n",
    "\n",
    "\n",
    "def make_inference(xs):\n",
    "    import base64, pickle\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    # Extract input lists\n",
    "    inference_lists = []\n",
    "    images_array = []\n",
    "    for x in xs:\n",
    "        inference_lists.append([x.tid, x.iid])\n",
    "        images_array.append(pickle.loads(base64.b64decode(x.i_ped.encode('UTF-8'))))\n",
    "    images_np = np.array(images_array)\n",
    "    # Load VGG16 model\n",
    "    vgg = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "    # Do inference\n",
    "    inference = vgg.predict(images_np)\n",
    "    # Add inference results to inference table lists\n",
    "    if len(inference_lists) != len(inference):\n",
    "        raise ValueError('The total of inference table lists is not ' +\n",
    "                         'the same as the total of inference result')\n",
    "    for i in range(len(inference_lists)):\n",
    "        inference_lists[i].append(\n",
    "            base64.b64encode(pickle.dumps(inference[i])).decode('UTF-8')\n",
    "        )\n",
    "\n",
    "    return iter(inference_lists)\n",
    "\n",
    "\n",
    "def main(spark):\n",
    "    from pyspark.sql import Row\n",
    "    # Input\n",
    "    image_ped_orc_pathfilename = \\\n",
    "        \"file:/home/jovyan/work/\" + \\\n",
    "        \"data/images_ped.orc\"\n",
    "    # Output\n",
    "    image_infr_orc_pathfilename = \\\n",
    "        \"file:/home/jovyan/work/\" + \\\n",
    "        \"data/images_infr_ss.orc\"\n",
    "    # Read input file\n",
    "    image_ped_dict_df = spark.read.orc(image_ped_orc_pathfilename)\n",
    "    image_ped_dict_rdd = image_ped_dict_df.rdd\n",
    "    print(\"        Number of Partitions:\", image_ped_dict_rdd.getNumPartitions())\n",
    "    # Perform inference\n",
    "    image_infr_list_rdd = image_ped_dict_rdd.mapPartitions(make_inference)\n",
    "    # Write output file\n",
    "    image_infr_dict_rdd = image_infr_list_rdd.map(lambda x: Row(tid=x[0],\n",
    "                                                                iid=x[1],\n",
    "                                                                pred=x[2]))\n",
    "    image_infr_dict_df = spark.createDataFrame(image_infr_dict_rdd)\n",
    "    image_infr_dict_df.write.save(image_infr_orc_pathfilename, format=\"orc\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(APP_NAME) \\\n",
    "        .getOrCreate()\n",
    "    main(spark)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Don't forget to delete existing `images_infr_ss.orc` directory in `data/`. Following Spark implementation does not overwrite existing data but it will throw error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Initial run will take very long as the Spark code needs to download about 500 MB of VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-30 19:26:05 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2019-07-30 19:26:05 INFO  SparkContext:54 - Running Spark version 2.3.3\n",
      "2019-07-30 19:26:05 INFO  SparkContext:54 - Submitted application: bukalapak-core-ai.big-data-3v.volume-spark-img-clsf\n",
      "2019-07-30 19:26:05 INFO  SecurityManager:54 - Changing view acls to: jovyan\n",
      "2019-07-30 19:26:05 INFO  SecurityManager:54 - Changing modify acls to: jovyan\n",
      "2019-07-30 19:26:05 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2019-07-30 19:26:05 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2019-07-30 19:26:05 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "2019-07-30 19:26:05 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 37359.\n",
      "2019-07-30 19:26:05 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2019-07-30 19:26:05 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2019-07-30 19:26:05 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2019-07-30 19:26:05 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2019-07-30 19:26:05 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-b1b3e322-9ed6-42d0-a168-1594363b829f\n",
      "2019-07-30 19:26:05 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2019-07-30 19:26:05 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2019-07-30 19:26:05 INFO  log:192 - Logging initialized @1414ms\n",
      "2019-07-30 19:26:05 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "2019-07-30 19:26:05 INFO  Server:419 - Started @1464ms\n",
      "2019-07-30 19:26:05 INFO  AbstractConnector:278 - Started ServerConnector@2a5fe865{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2019-07-30 19:26:05 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@e13c049{/jobs,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@12467089{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2030d7a7{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@38ec3a73{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@190fc7df{/stages,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3fa4a6b6{/stages/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6feff8{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24d4f78e{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bbbccc3{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@49310dc2{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@c14bb5e{/storage,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@fff45d5{/storage/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@aa74d3d{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b391eb6{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62de92c4{/environment,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@17faa2a7{/environment/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2784dbc0{/executors,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24cbbd28{/executors/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@321553c7{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@70b820b8{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71e3afb4{/static,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1bead211{/,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@129ea728{/api,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@78915363{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3ba897b8{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:05 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://00009bd93b41:4040\n",
      "2019-07-30 19:26:05 INFO  SparkContext:54 - Added file file:/home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py at file:/home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py with timestamp 1564514765997\n",
      "2019-07-30 19:26:05 INFO  Utils:54 - Copying /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py to /tmp/spark-27f235d4-a27f-47e7-96de-1b925fb3cc39/userFiles-49c1c443-b019-4a4e-bdaf-8929968847a1/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py\n",
      "2019-07-30 19:26:06 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2019-07-30 19:26:06 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36499.\n",
      "2019-07-30 19:26:06 INFO  NettyBlockTransferService:54 - Server created on 00009bd93b41:36499\n",
      "2019-07-30 19:26:06 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2019-07-30 19:26:06 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 00009bd93b41, 36499, None)\n",
      "2019-07-30 19:26:06 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 00009bd93b41:36499 with 366.3 MB RAM, BlockManagerId(driver, 00009bd93b41, 36499, None)\n",
      "2019-07-30 19:26:06 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 00009bd93b41, 36499, None)\n",
      "2019-07-30 19:26:06 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 00009bd93b41, 36499, None)\n",
      "2019-07-30 19:26:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@16cf5685{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:06 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/work/spark-warehouse/').\n",
      "2019-07-30 19:26:06 INFO  SharedState:54 - Warehouse path is 'file:/home/jovyan/work/spark-warehouse/'.\n",
      "2019-07-30 19:26:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@c1de5e9{/SQL,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4e53f5d0{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5cd64ada{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@74a3d78b{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6eddad77{/static/sql,null,AVAILABLE,@Spark}\n",
      "2019-07-30 19:26:06 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "2019-07-30 19:26:07 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-07-30 19:26:07 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2019-07-30 19:26:07 INFO  FileSourceStrategy:54 - Output Data Schema: struct<i_ped: string, iid: string, l: string, tid: string ... 2 more fields>\n",
      "2019-07-30 19:26:07 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-07-30 19:26:08 INFO  CodeGenerator:54 - Code generated in 126.819049 ms\n",
      "2019-07-30 19:26:08 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 277.6 KB, free 366.0 MB)\n",
      "2019-07-30 19:26:08 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.4 KB, free 366.0 MB)\n",
      "2019-07-30 19:26:08 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 00009bd93b41:36499 (size: 23.4 KB, free: 366.3 MB)\n",
      "2019-07-30 19:26:08 INFO  SparkContext:54 - Created broadcast 0 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2019-07-30 19:26:08 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "        Number of Partitions: 2\n",
      "2019-07-30 19:26:08 INFO  SparkContext:54 - Starting job: runJob at PythonRDD.scala:152\n",
      "2019-07-30 19:26:08 INFO  DAGScheduler:54 - Got job 0 (runJob at PythonRDD.scala:152) with 1 output partitions\n",
      "2019-07-30 19:26:08 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (runJob at PythonRDD.scala:152)\n",
      "2019-07-30 19:26:08 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-07-30 19:26:08 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-07-30 19:26:08 INFO  DAGScheduler:54 - Submitting ResultStage 0 (PythonRDD[4] at RDD at PythonRDD.scala:52), which has no missing parents\n",
      "2019-07-30 19:26:08 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 11.9 KB, free 366.0 MB)\n",
      "2019-07-30 19:26:08 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.1 KB, free 366.0 MB)\n",
      "2019-07-30 19:26:08 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 00009bd93b41:36499 (size: 7.1 KB, free: 366.3 MB)\n",
      "2019-07-30 19:26:08 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\n",
      "2019-07-30 19:26:08 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at RDD at PythonRDD.scala:52) (first 15 tasks are for partitions Vector(0))\n",
      "2019-07-30 19:26:08 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks\n",
      "2019-07-30 19:26:08 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8373 bytes)\n",
      "2019-07-30 19:26:08 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)\n",
      "2019-07-30 19:26:08 INFO  Executor:54 - Fetching file:/home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py with timestamp 1564514765997\n",
      "2019-07-30 19:26:08 INFO  Utils:54 - /home/jovyan/work/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py has been previously copied to /tmp/spark-27f235d4-a27f-47e7-96de-1b925fb3cc39/userFiles-49c1c443-b019-4a4e-bdaf-8929968847a1/bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py\n",
      "2019-07-30 19:26:08 INFO  FileScanRDD:54 - Reading File path: file:///home/jovyan/work/data/images_ped.orc/part-00000-6ba5dcd1-092d-436c-8384-c8abe7b76be4-c000.snappy.orc, range: 0-1797187, partition values: [empty row]\n",
      "2019-07-30 19:26:08 INFO  CodeGenerator:54 - Code generated in 16.597565 ms\n",
      "2019-07-30 19:26:08 INFO  ReaderImpl:531 - Reading ORC rows from file:/home/jovyan/work/data/images_ped.orc/part-00000-6ba5dcd1-092d-436c-8384-c8abe7b76be4-c000.snappy.orc with {include: [true, true, true, true, true], offset: 0, length: 1797187}\n",
      "2019-07-30 19:26:09 INFO  RecordReaderFactory:90 - Schema is not specified on read. Using file schema.\n",
      "2019-07-30 19:26:12 INFO  PythonRunner:54 - Times: total = 4272, boot = 325, init = 170, finish = 3777\n",
      "2019-07-30 19:26:12 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 7585 bytes result sent to driver\n",
      "2019-07-30 19:26:12 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 4371 ms on localhost (executor driver) (1/1)\n",
      "2019-07-30 19:26:12 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2019-07-30 19:26:12 INFO  PythonAccumulatorV2:54 - Connected to AccumulatorServer at host: 127.0.0.1 port: 41473\n",
      "2019-07-30 19:26:12 INFO  DAGScheduler:54 - ResultStage 0 (runJob at PythonRDD.scala:152) finished in 4.431 s\n",
      "2019-07-30 19:26:12 INFO  DAGScheduler:54 - Job 0 finished: runJob at PythonRDD.scala:152, took 4.455147 s\n",
      "2019-07-30 19:26:12 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2019-07-30 19:26:12 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2019-07-30 19:26:12 INFO  SparkContext:54 - Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "2019-07-30 19:26:12 INFO  DAGScheduler:54 - Got job 1 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2019-07-30 19:26:12 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (save at NativeMethodAccessorImpl.java:0)\n",
      "2019-07-30 19:26:12 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-07-30 19:26:12 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-07-30 19:26:12 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2019-07-30 19:26:13 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 155.5 KB, free 365.8 MB)\n",
      "2019-07-30 19:26:13 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 58.1 KB, free 365.8 MB)\n",
      "2019-07-30 19:26:13 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 00009bd93b41:36499 (size: 58.1 KB, free: 366.2 MB)\n",
      "2019-07-30 19:26:13 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1039\n",
      "2019-07-30 19:26:13 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2019-07-30 19:26:13 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 2 tasks\n",
      "2019-07-30 19:26:13 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8373 bytes)\n",
      "2019-07-30 19:26:13 INFO  TaskSetManager:54 - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 8373 bytes)\n",
      "2019-07-30 19:26:13 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)\n",
      "2019-07-30 19:26:13 INFO  Executor:54 - Running task 1.0 in stage 1.0 (TID 2)\n",
      "2019-07-30 19:26:13 INFO  FileScanRDD:54 - Reading File path: file:///home/jovyan/work/data/images_ped.orc/part-00001-6ba5dcd1-092d-436c-8384-c8abe7b76be4-c000.snappy.orc, range: 0-1797175, partition values: [empty row]\n",
      "2019-07-30 19:26:13 INFO  FileScanRDD:54 - Reading File path: file:///home/jovyan/work/data/images_ped.orc/part-00000-6ba5dcd1-092d-436c-8384-c8abe7b76be4-c000.snappy.orc, range: 0-1797187, partition values: [empty row]\n",
      "2019-07-30 19:26:13 INFO  CodeGenerator:54 - Code generated in 24.524601 ms\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 19\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 7\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 14\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 17\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 15\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 13\n",
      "2019-07-30 19:26:13 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 21\n",
      "2019-07-30 19:26:13 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 29\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 30\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 9\n",
      "2019-07-30 19:26:13 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2019-07-30 19:26:13 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 20\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 11\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 28\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 22\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 6\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 24\n",
      "2019-07-30 19:26:13 INFO  ReaderImpl:531 - Reading ORC rows from file:/home/jovyan/work/data/images_ped.orc/part-00000-6ba5dcd1-092d-436c-8384-c8abe7b76be4-c000.snappy.orc with {include: [true, true, true, true, true], offset: 0, length: 1797187}\n",
      "2019-07-30 19:26:13 INFO  RecordReaderFactory:90 - Schema is not specified on read. Using file schema.\n",
      "2019-07-30 19:26:13 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 00009bd93b41:36499 in memory (size: 7.1 KB, free: 366.2 MB)\n",
      "2019-07-30 19:26:13 INFO  ReaderImpl:531 - Reading ORC rows from file:/home/jovyan/work/data/images_ped.orc/part-00001-6ba5dcd1-092d-436c-8384-c8abe7b76be4-c000.snappy.orc with {include: [true, true, true, true, true], offset: 0, length: 1797175}\n",
      "2019-07-30 19:26:13 INFO  RecordReaderFactory:90 - Schema is not specified on read. Using file schema.\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 12\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 10\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 25\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 18\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 8\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 27\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 16\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 26\n",
      "2019-07-30 19:26:13 INFO  ContextCleaner:54 - Cleaned accumulator 23\n",
      "2019-07-30 19:26:15 INFO  PythonRunner:54 - Times: total = 2522, boot = -166, init = 245, finish = 2443\n",
      "2019-07-30 19:26:15 INFO  FileOutputCommitter:535 - Saved output of task 'attempt_20190730192613_0001_m_000001_2' to file:/home/jovyan/work/data/images_infr_ss.orc/_temporary/0/task_20190730192613_0001_m_000001\n",
      "2019-07-30 19:26:15 INFO  SparkHadoopMapRedUtil:54 - attempt_20190730192613_0001_m_000001_2: Committed\n",
      "2019-07-30 19:26:15 INFO  Executor:54 - Finished task 1.0 in stage 1.0 (TID 2). 2986 bytes result sent to driver\n",
      "2019-07-30 19:26:15 INFO  TaskSetManager:54 - Finished task 1.0 in stage 1.0 (TID 2) in 2636 ms on localhost (executor driver) (1/2)\n",
      "2019-07-30 19:26:17 INFO  PythonRunner:54 - Times: total = 4071, boot = 3, init = 48, finish = 4020\n",
      "2019-07-30 19:26:17 INFO  FileOutputCommitter:535 - Saved output of task 'attempt_20190730192613_0001_m_000000_1' to file:/home/jovyan/work/data/images_infr_ss.orc/_temporary/0/task_20190730192613_0001_m_000000\n",
      "2019-07-30 19:26:17 INFO  SparkHadoopMapRedUtil:54 - attempt_20190730192613_0001_m_000000_1: Committed\n",
      "2019-07-30 19:26:17 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 1). 2986 bytes result sent to driver\n",
      "2019-07-30 19:26:17 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 4133 ms on localhost (executor driver) (2/2)\n",
      "2019-07-30 19:26:17 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2019-07-30 19:26:17 INFO  DAGScheduler:54 - ResultStage 1 (save at NativeMethodAccessorImpl.java:0) finished in 4.156 s\n",
      "2019-07-30 19:26:17 INFO  DAGScheduler:54 - Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 4.158664 s\n",
      "2019-07-30 19:26:17 INFO  FileFormatWriter:54 - Job null committed.\n",
      "2019-07-30 19:26:17 INFO  FileFormatWriter:54 - Finished processing stats for job null.\n",
      "2019-07-30 19:26:17 INFO  AbstractConnector:318 - Stopped Spark@2a5fe865{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2019-07-30 19:26:17 INFO  SparkUI:54 - Stopped Spark web UI at http://00009bd93b41:4040\n",
      "2019-07-30 19:26:17 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2019-07-30 19:26:17 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2019-07-30 19:26:17 INFO  BlockManager:54 - BlockManager stopped\n",
      "2019-07-30 19:26:17 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2019-07-30 19:26:17 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2019-07-30 19:26:17 INFO  SparkContext:54 - Successfully stopped SparkContext\n",
      "2019-07-30 19:26:18 INFO  ShutdownHookManager:54 - Shutdown hook called\n",
      "2019-07-30 19:26:18 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-e51b20a5-5fe6-4e5b-9dd8-254c27f002b6\n",
      "2019-07-30 19:26:18 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-27f235d4-a27f-47e7-96de-1b925fb3cc39/pyspark-621e6cad-f2a4-4c43-8a13-59a9369b1240\n",
      "2019-07-30 19:26:18 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-27f235d4-a27f-47e7-96de-1b925fb3cc39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 19:26:10.285770 140634678175552 deprecation.py:506] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "2019-07-30 19:26:11.537662: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-07-30 19:26:11.558607: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\n",
      "2019-07-30 19:26:11.559128: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5630de167ef0 executing computations on platform Host. Devices:\n",
      "2019-07-30 19:26:11.559147: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-07-30 19:26:11.589896: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 19:26:14.350521 140634678175552 deprecation.py:506] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "2019-07-30 19:26:15.817183: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-07-30 19:26:15.838569: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\n",
      "2019-07-30 19:26:15.839007: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5630e10daab0 executing computations on platform Host. Devices:\n",
      "2019-07-30 19:26:15.839029: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-07-30 19:26:15.870137: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/spark/bin/spark-submit \\\n",
    "    --executor-memory 2g --executor-cores 1 --num-executors 2 \\\n",
    "    bukalapak-core-ai.big-data-3v.volume-spark-img-clsf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Settings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_PROJECT_URL = '/home/jovyan/work/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Spark Context__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set config to run Spark locally with 2 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"bukalapak-core-ai.big-data-3v.volume-spark-img-clsf\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.master\", \"local[2]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read Inference Result__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_infr_orc_directory = \"{0}data/images_infr_ss.orc\".format(LOCAL_PROJECT_URL)\n",
    "image_infr_orc_pathfilename = \"file:{0}\".format(image_infr_orc_directory)\n",
    "image_infr_dict_df = spark.read.orc(image_infr_orc_pathfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_infr_dict_rdd = image_infr_dict_df.rdd\n",
    "inference_lists = image_infr_dict_rdd.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/images/jacket copy 0.png'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gANjbnVtcHkuY29yZS5tdWx0aWFycmF5Cl9yZWNvbnN0cnVjdApxAGNudW1weQpuZGFycmF5CnEBSwCFcQJDAWJxA4dxBFJxBShL'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_lists[1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pbs_to_infr_tensor(pbs):\n",
    "    return np.array(\n",
    "        [list(pickle.loads(base64.b64decode(pbs.encode('UTF-8'))))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n04370456', 'sweatshirt', 0.971396),\n",
       "  ('n04599235', 'wool', 0.015504221),\n",
       "  ('n03595614', 'jersey', 0.005360415),\n",
       "  ('n02963159', 'cardigan', 0.0013214782),\n",
       "  ('n03594734', 'jean', 0.0011332377)]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.applications.vgg16.decode_predictions(\n",
    "    convert_pbs_to_infr_tensor(inference_lists[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:\n",
    "```\n",
    "[[('n04370456', 'sweatshirt', 0.971396),\n",
    "  ('n04599235', 'wool', 0.015504221),\n",
    "  ('n03595614', 'jersey', 0.005360415),\n",
    "  ('n02963159', 'cardigan', 0.0013214782),\n",
    "  ('n03594734', 'jean', 0.0011332377)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Close Spark Context__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. K Simonyan, A Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. CoRR, abs/1409.1556. http://arxiv.org/abs/1409.1556."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
